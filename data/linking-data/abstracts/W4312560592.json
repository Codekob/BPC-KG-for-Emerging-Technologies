{"The": [0, 78], "vision": [1], "community": [2], "is": [3, 85], "witnessing": [4], "a": [5, 62, 113], "modeling": [6, 147], "shift": [7], "from": [8], "CNNs": [9], "to": [10, 61, 67, 99], "Transformers,": [11, 58], "where": [12], "pure": [13], "Transformer": [14, 33, 91], "architectures": [15], "have": [16], "attained": [17], "top": [18], "accuracy": [19, 111, 126, 132, 150], "on": [20, 32, 112, 121, 127, 133, 151], "the": [21, 40, 81, 89, 94, 101], "major": [22], "video": [23, 27, 57, 83, 117], "recognition": [24, 118, 123], "benchmarks.": [25], "These": [26], "models": [28], "are": [29], "all": [30], "built": [31], "layers": [34], "that": [35], "globally": [36, 73], "connect": [37], "patches": [38], "across": [39], "spatial": [41], "and": [42, 129, 140, 145], "temporal": [43, 146], "dimensions.": [44], "In": [45], "this": [46], "paper,": [47], "we": [48], "instead": [49], "advocate": [50], "an": [51], "inductive": [52], "bias": [53], "of": [54, 80, 103, 116], "locality": [55, 79], "in": [56], "which": [59, 70], "leads": [60], "better": [63], "speed-accuracy": [64], "trade-off": [65], "compared": [66], "previous": [68], "approaches": [69], "compute": [71], "self-attention": [72], "even": [74], "with": [75, 135], "spatial-temporal": [76], "factorization.": [77], "proposed": [82], "architecture": [84], "realized": [86], "by": [87], "adapting": [88], "Swin": [90], "designed": [92], "for": [93], "image": [95, 105], "domain,": [96], "while": [97], "continuing": [98], "leverage": [100], "power": [102], "pre-trained": [104], "models.": [106], "Our": [107], "approach": [108], "achieves": [109], "state-of-the-art": [110], "broad": [114], "range": [115], "benchmarks,": [119], "including": [120], "action": [122], "(84.9": [124], "top-l": [125, 131, 149], "Kinetics-400": [128], "85.9": [130], "Kinetics-600": [134], "~20\u00d7": [136], "less": [137], "pre-training": [138], "data": [139], "~3\u00d7": [141], "smaller": [142], "model": [143], "size)": [144], "(69.6": [148], "Something-Something": [152], "v2).": [153]}