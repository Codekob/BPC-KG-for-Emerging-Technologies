{"Transformers": [0], "have": [1], "shown": [2], "great": [3], "potential": [4], "in": [5, 30, 82, 182], "computer": [6, 112], "vision": [7, 113, 127, 193], "tasks.": [8, 114, 194], "A": [9], "common": [10], "belief": [11], "is": [12, 66, 178], "their": [13, 21], "attention-based": [14, 28], "token": [15, 63, 95, 167, 211], "mixer": [16, 64, 212], "module": [17, 29, 81], "contributes": [18], "most": [19], "to": [20, 69, 91, 152, 203], "competence.": [22], "However,": [23], "recent": [24, 187], "works": [25], "show": [26], "the": [27, 39, 53, 57, 61, 70, 79, 101, 154, 166, 171, 179, 210], "transformers": [31, 83, 163], "can": [32], "be": [33], "replaced": [34], "by": [35, 131], "spatial": [36, 88], "MLPs": [37], "and": [38, 138, 149, 189], "resulted": [40], "models": [41, 191], "still": [42], "perform": [43], "quite": [44], "well.": [45], "Based": [46, 169], "on": [47, 110, 117, 170, 192, 209], "this": [48], "observation,": [49], "we": [50, 76, 98, 174], "hypothesize": [51], "that": [52, 100, 176], "general": [54, 159], "architecture": [55, 160, 227], "of": [56, 60, 144, 156, 207], "transformers,": [58], "instead": [59, 206], "specific": [62], "module,": [65], "more": [67, 199], "essential": [68], "model's": [71], "performance.": [72], "To": [73], "verify": [74], "this,": [75], "deliberately": [77], "replace": [78], "attention": [80], "with": [84, 134], "an": [85], "embarrassingly": [86], "simple": [87], "pooling": [89], "operator": [90], "conduct": [92], "only": [93], "basic": [94], "mixing.": [96], "Surprisingly,": [97], "observe": [99], "derived": [102], "model,": [103], "termed": [104], "as": [105, 220], "PoolFormer,": [106], "achieves": [107, 120], "competitive": [108], "performance": [109], "multiple": [111], "For": [115], "example,": [116], "ImageNet-1K,": [118], "PoolFormer": [119, 217], "82.1": [121], "%": [122], "top-1": [123], "accuracy,": [124], "surpassing": [125], "well-tuned": [126], "transformer/MLP-like": [128], "baselines": [129], "DeiT-B/ResMLP-B24": [130], "0.3%/1.1%": [132], "accuracy": [133], "35%/52%": [135], "fewer": [136, 140], "parameters": [137], "49%/61%": [139], "MACs.": [141], "The": [142], "effectiveness": [143], "Pool-Former": [145], "verifies": [146], "our": [147, 215], "hypothesis": [148], "urges": [150], "us": [151], "initiate": [153], "concept": [155], "\"MetaFormer\",": [157], "a": [158, 221], "abstracted": [161], "from": [162], "without": [164], "specifying": [165], "mixer.": [168], "extensive": [172], "experiments,": [173], "argue": [175], "MetaFormer": [177, 205, 226], "key": [180], "player": [181], "achieving": [183], "superior": [184], "results": [185], "for": [186, 198, 224], "transformer": [188], "MLP-like": [190], "This": [195], "work": [196], "calls": [197], "future": [200, 225], "research": [201], "dedicated": [202], "improving": [204], "focusing": [208], "modules.": [213], "Additionally,": [214], "proposed": [216], "could": [218], "serve": [219], "starting": [222], "baseline": [223], "design.": [228]}