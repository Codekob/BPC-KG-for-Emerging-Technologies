{"Foundation": [0], "models,": [1, 36], "now": [2], "powering": [3], "most": [4], "of": [5, 75, 97, 133, 217], "the": [6, 17, 92, 98, 107, 116, 122, 131, 218], "exciting": [7], "applications": [8], "in": [9, 142, 176, 228], "deep": [10], "learning,": [11], "are": [12], "almost": [13], "universally": [14], "based": [15], "on": [16, 51, 63, 121, 183], "Transformer": [18], "architecture": [19, 156], "and": [20, 34, 37, 85, 173, 179, 207, 221, 230], "its": [21, 180, 225], "core": [22], "attention": [23, 62, 158], "module.": [24], "Many": [25], "subquadratic-time": [26], "architectures": [27], "such": [28, 66, 76, 203], "as": [29, 59, 61, 67, 204], "linear": [30, 174], "attention,": [31], "gated": [32], "convolution": [33], "recurrent": [35, 143], "structured": [38], "state": [39], "space": [40], "models": [41, 77], "(SSMs)": [42], "have": [43, 56], "been": [44], "developed": [45], "to": [46, 81, 109, 187], "address": [47], "Transformers'": [48], "computational": [49], "inefficiency": [50], "long": [52], "sequences,": [53], "but": [54], "they": [55], "not": [57], "performed": [58], "well": [60], "important": [64], "modalities": [65, 202], "language.": [68], "We": [69, 145], "identify": [70], "that": [71], "a": [72, 138, 151, 191], "key": [73], "weakness": [74, 102], "is": [78], "their": [79, 101], "inability": [80], "perform": [82], "content-based": [83], "reasoning,": [84], "make": [86], "several": [87, 201], "improvements.": [88], "First,": [89], "simply": [90], "letting": [91], "SSM": [93], "parameters": [94], "be": [95], "functions": [96], "input": [99], "addresses": [100], "with": [103], "discrete": [104], "modalities,": [105], "allowing": [106], "model": [108, 194, 214], "selectively": [110], "propagate": [111], "or": [112, 159], "forget": [113], "information": [114], "along": [115], "sequence": [117, 177, 193], "length": [118], "dimension": [119], "depending": [120], "current": [123], "token.": [124], "Second,": [125], "even": [126, 160], "though": [127], "this": [128], "change": [129], "prevents": [130], "use": [132], "efficient": [134], "convolutions,": [135], "we": [136], "design": [137], "hardware-aware": [139], "parallel": [140], "algorithm": [141], "mode.": [144], "integrate": [146], "these": [147], "selective": [148], "SSMs": [149], "into": [150], "simplified": [152], "end-to-end": [153], "neural": [154], "network": [155], "without": [157], "MLP": [161], "blocks": [162], "(Mamba).": [163], "Mamba": [164, 196], "enjoys": [165], "fast": [166], "inference": [167], "(5$\\times$": [168], "higher": [169], "throughput": [170], "than": [171], "Transformers)": [172], "scaling": [175], "length,": [178], "performance": [181, 199], "improves": [182], "real": [184], "data": [185], "up": [186], "million-length": [188], "sequences.": [189], "As": [190], "general": [192], "backbone,": [195], "achieves": [197], "state-of-the-art": [198], "across": [200], "language,": [205], "audio,": [206], "genomics.": [208], "On": [209], "language": [210], "modeling,": [211], "our": [212], "Mamba-3B": [213], "outperforms": [215], "Transformers": [216, 223], "same": [219], "size": [220], "matches": [222], "twice": [224], "size,": [226], "both": [227], "pretraining": [229], "downstream": [231], "evaluation.": [232]}