{"Abstract": [0], "While": [1, 106], "originally": [2], "designed": [3], "for": [4, 30, 53, 155, 167, 178, 195], "natural": [5], "language": [6], "processing": [7], "tasks,": [8, 126], "the": [9, 22, 47, 196], "self-attention": [10, 32, 87], "mechanism": [11], "has": [12], "recently": [13], "taken": [14], "various": [15, 125], "computer": [16, 34], "vision": [17, 121], "areas": [18], "by": [19], "storm.": [20], "However,": [21], "2D": [23, 44], "nature": [24], "of": [25], "images": [26, 38], "brings": [27], "three": [28], "challenges": [29], "applying": [31], "in": [33, 86, 124], "vision:": [35], "(1)": [36], "treating": [37], "as": [39], "1D": [40], "sequences": [41], "neglects": [42], "their": [43], "structures;": [45], "(2)": [46], "quadratic": [48], "complexity": [49], "is": [50, 200], "too": [51], "expensive": [52], "high-resolution": [54], "images;": [55], "(3)": [56], "it": [57], "only": [58], "captures": [59], "spatial": [60], "adaptability": [61], "but": [62], "ignores": [63], "channel": [64], "adaptability.": [65], "In": [66], "this": [67], "paper,": [68], "we": [69, 93], "propose": [70], "a": [71, 95, 186, 190], "novel": [72, 187], "linear": [73], "attention": [74, 78], "named": [75], "large": [76], "kernel": [77], "(LKA)": [79], "to": [80], "enable": [81], "self-adaptive": [82], "and": [83, 120, 148, 189], "long-range": [84], "correlations": [85], "while": [88], "avoiding": [89], "its": [90], "shortcomings.": [91], "Furthermore,": [92], "present": [94], "neural": [96, 117], "network": [97], "based": [98], "on": [99, 145, 170, 181], "LKA,": [100], "namely": [101], "Visual": [102], "Attention": [103], "Network": [104], "(VAN).": [105], "extremely": [107], "simple,": [108], "VAN": [109], "achieves": [110, 142], "comparable": [111], "results": [112], "with": [113], "similar": [114], "size": [115], "convolutional": [116], "networks": [118], "(CNNs)": [119], "transformers": [122], "(ViTs)": [123], "including": [127], "image": [128], "classification,": [129], "object": [130, 179], "detection,": [131], "semantic": [132, 168], "segmentation,": [133, 135], "panoptic": [134, 156], "pose": [136], "estimation,": [137], "etc.": [138], "For": [139], "example,": [140], "VAN-B6": [141], "87.8%": [143], "accuracy": [144], "ImageNet": [146], "benchmark,": [147, 172], "sets": [149], "new": [150], "state-of-the-art": [151], "performance": [152], "(58.2": [153], "PQ)": [154], "segmentation.": [157], "Besides,": [158], "VAN-B2": [159], "surpasses": [160], "Swin-T": [161], "4": [162], "mIoU": [163], "(50.1": [164], "vs.": [165, 176], "46.1)": [166], "segmentation": [169], "ADE20K": [171], "2.6": [173], "AP": [174], "(48.8": [175], "46.2)": [177], "detection": [180], "COCO": [182], "dataset.": [183], "It": [184], "provides": [185], "method": [188], "simple": [191], "yet": [192], "strong": [193], "baseline": [194], "community.": [197], "The": [198], "code": [199], "available": [201], "at": [202], "https://github.com/Visual-Attention-Network": [203], ".": [204]}