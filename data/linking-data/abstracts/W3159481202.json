{"In": [0], "this": [1, 32], "paper,": [2], "we": [3, 37, 118], "question": [4], "if": [5], "self-supervised": [6, 29, 43, 113], "learning": [7], "provides": [8], "new": [9], "properties": [10], "to": [11, 20, 31], "Vision": [12], "Transformer": [13], "(ViT)": [14], "[16]": [15], "that": [16, 27], "stand": [17], "out": [18], "compared": [19], "convolutional": [21], "networks": [22], "(convnets).": [23], "Beyond": [24], "the": [25, 39, 50, 89, 99, 130], "fact": [26], "adapting": [28], "methods": [30], "architecture": [33], "works": [34], "particularly": [35], "well,": [36], "make": [38], "following": [40], "observations:": [41], "first,": [42], "ViT": [44], "features": [45, 70], "contain": [46], "explicit": [47], "information": [48], "about": [49], "semantic": [51], "segmentation": [52], "of": [53, 91, 101, 123], "an": [54], "image,": [55], "which": [56, 117], "does": [57], "not": [58], "emerge": [59], "as": [60, 120], "clearly": [61], "with": [62, 66, 81, 104, 125, 145], "supervised": [63], "ViTs,": [64], "nor": [65], "convnets.": [67], "Second,": [68], "these": [69], "are": [71], "also": [72, 87], "excellent": [73], "k-NN": [74], "classifiers,": [75], "reaching": [76], "78.3%": [77], "top-1": [78, 139], "on": [79, 140], "ImageNet": [80, 141], "a": [82, 111, 121], "small": [83, 102], "ViT.": [84], "Our": [85], "study": [86], "underlines": [88], "importance": [90], "momentum": [92], "encoder": [93], "[26],": [94], "multi-crop": [95], "training": [96], "[9],": [97], "and": [98, 134], "use": [100], "patches": [103], "ViTs.": [105], "We": [106, 128], "implement": [107], "our": [108], "findings": [109], "into": [110], "simple": [112], "method,": [114], "called": [115], "DINO,": [116], "interpret": [119], "form": [122], "self-distillation": [124], "no": [126], "labels.": [127], "show": [129], "synergy": [131], "between": [132], "DINO": [133], "ViTs": [135], "by": [136], "achieving": [137], "80.1%": [138], "in": [142], "linear": [143], "evaluation": [144], "ViT-Base.": [146]}