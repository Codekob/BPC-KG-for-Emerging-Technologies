{"Recently": [0], "Transformer": [1, 87], "and": [2, 54, 60, 88, 110], "Convolution": [3], "neural": [4, 20, 52], "network": [5], "(CNN)": [6], "based": [7, 90], "models": [8, 23, 91], "have": [9], "shown": [10], "promising": [11], "results": [12], "in": [13, 67], "Automatic": [14], "Speech": [15], "Recognition": [16], "(ASR),": [17], "outperforming": [18], "Recurrent": [19], "networks": [21, 53], "(RNNs).Transformer": [22], "are": [24], "good": [25], "at": [26], "capturing": [27], "content-based": [28], "global": [29, 61], "interactions,": [30], "while": [31], "CNNs": [32], "exploit": [33], "local": [34, 59], "features": [35], "effectively.In": [36], "this": [37, 71], "work,": [38], "we": [39, 73], "achieve": [40], "the": [41, 75, 85, 95], "best": [42], "of": [43, 63, 104, 122, 127], "both": [44, 58], "worlds": [45], "by": [46], "studying": [47], "how": [48], "to": [49, 56], "combine": [50], "convolution": [51], "transformers": [55], "model": [57, 101, 109, 115, 126], "dependencies": [62], "an": [64, 112], "audio": [65], "sequence": [66], "a": [68, 107, 124], "parameter-efficient": [69], "way.To": [70], "regard,": [72], "propose": [74], "convolution-augmented": [76], "transformer": [77], "for": [78], "speech": [79], "recognition,": [80], "named": [81], "Conformer.Conformer": [82], "significantly": [83], "outperforms": [84], "previous": [86], "CNN": [89], "achieving": [92], "state-of-the-art": [93], "accuracies.On": [94], "widely": [96], "used": [97], "LibriSpeech": [98], "benchmark,": [99], "our": [100], "achieves": [102], "WER": [103], "2.1%/4.3%without": [105], "using": [106], "language": [108, 114], "1.9%/3.9%with": [111], "external": [113], "on": [116], "test/testother.We": [117], "also": [118], "observe": [119], "competitive": [120], "performance": [121], "2.7%/6.3%with": [123], "small": [125], "only": [128], "10M": [129], "parameters.": [130]}