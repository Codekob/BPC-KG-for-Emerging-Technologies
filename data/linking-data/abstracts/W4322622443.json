{"Abstract": [0], "Background:": [1], "Natural": [2], "language": [3], "processing": [4], "models": [5], "such": [6], "as": [7, 52, 228], "ChatGPT": [8, 31, 219], "can": [9], "generate": [10], "text-based": [11], "content": [12], "and": [13, 24, 28, 88, 110, 131, 141, 151, 163, 171, 176, 183, 240, 248], "are": [14, 243], "poised": [15], "to": [16, 70, 83, 97, 224, 245], "become": [17], "a": [18], "major": [19], "information": [20, 223], "source": [21], "in": [22], "medicine": [23], "beyond.": [25], "The": [26, 64], "accuracy": [27, 74, 124, 166], "completeness": [29, 89, 146], "of": [30, 137, 156, 202], "for": [32, 73, 181, 249], "medical": [33, 46, 226], "queries": [34, 227], "is": [35], "not": [36], "known.": [37], "Methods:": [38], "Thirty-three": [39], "physicians": [40, 65], "across": [41], "17": [42], "specialties": [43], "generated": [44, 220], "284": [45], "questions": [47, 72, 121, 159, 185, 199], "that": [48], "they": [49], "subjectively": [50], "classified": [51], "easy,": [53, 161], "medium,": [54, 162], "or": [55, 61, 115], "hard": [56], "with": [57, 107, 134, 153, 200, 210, 235], "either": [58], "binary": [59, 182], "(yes/no)": [60], "descriptive": [62, 108, 184], "answers.": [63], "then": [66], "graded": [67], "ChatGPT-generated": [68], "answers": [69], "these": [71], "(6-point": [75], "Likert": [76, 91], "scale;": [77, 92], "range": [78, 93], "1": [79, 94], "\u2013": [80, 85, 95], "completely": [81, 86, 130, 132, 143], "incorrect": [82], "6": [84, 189], "correct)": [87, 133], "(3-point": [90], "incomplete": [96], "3": [98, 149], "-": [99], "complete": [100], "plus": [101], "additional": [102], "context).": [103], "Scores": [104], "were": [105, 168, 186, 205], "summarized": [106], "statistics": [109], "compared": [111], "using": [112], "Mann-Whitney": [113], "U": [114], "Kruskal-Wallis": [116], "testing.": [117], "Results:": [118], "Across": [119], "all": [120], "(n=284),": [122], "median": [123, 165], "score": [125, 136, 147, 155], "was": [126, 148], "5.5": [127], "(between": [128, 139], "almost": [129, 142], "mean": [135, 154, 192], "4.8": [138], "mostly": [140], "correct).": [144], "Median": [145], "(complete": [150], "comprehensive)": [152], "2.5.": [157], "For": [158], "rated": [160], "hard,": [164], "scores": [167, 180, 201], "6,": [169], "5.5,": [170], "5": [172], "(mean": [173], "5.0,": [174], "4.7,": [175], "4.6;": [177], "p=0.05).": [178], "Accuracy": [179], "similar": [187], "(median": [188, 213], "vs.": [190, 194, 215], "5;": [191], "4.9": [193], "4.7;": [195], "p=0.07).": [196], "Of": [197], "36": [198], "1-2,": [203], "34": [204], "re-queried/re-graded": [206], "8-17": [207], "days": [208], "later": [209], "substantial": [211], "improvement": [212], "2": [214], "4;": [216], "p&lt;0.01).": [217], "Conclusions:": [218], "largely": [221], "accurate": [222], "diverse": [225], "judged": [229], "by": [230], "academic": [231], "physician": [232], "specialists": [233], "although": [234], "important": [236], "limitations.": [237], "Further": [238], "research": [239], "model": [241], "development": [242], "needed": [244], "correct": [246], "inaccuracies": [247], "validation.": [250]}