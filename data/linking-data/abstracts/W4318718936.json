{"The": [0, 57, 69], "cost": [1], "of": [2, 13, 121], "vision-and-language": [3], "pre-training": [4, 24, 29], "has": [5], "become": [6], "increasingly": [7], "prohibitive": [8], "due": [9], "to": [10], "end-to-end": [11], "training": [12], "large-scale": [14], "models.": [15, 40], "This": [16], "paper": [17], "proposes": [18], "BLIP-2,": [19], "a": [20, 47, 65, 77], "generic": [21], "and": [22, 36], "efficient": [23], "strategy": [25], "that": [26, 125], "bootstraps": [27, 60, 72], "vision-language": [28, 61, 87], "from": [30, 64, 76], "off-the-shelf": [31], "frozen": [32, 37, 66, 78], "pre-trained": [33, 53], "image": [34, 67], "encoders": [35], "large": [38], "language": [39, 79, 129], "BLIP-2": [41, 81], "bridges": [42], "the": [43, 117], "modality": [44], "gap": [45], "with": [46, 109], "lightweight": [48], "Querying": [49], "Transformer,": [50], "which": [51], "is": [52], "in": [54], "two": [55], "stages.": [56], "first": [58], "stage": [59, 71], "representation": [62], "learning": [63, 75], "encoder.": [68], "second": [70], "vision-to-language": [73], "generative": [74], "model.": [80], "achieves": [82], "state-of-the-art": [83], "performance": [84], "on": [85, 106], "various": [86], "tasks,": [88], "despite": [89], "having": [90], "significantly": [91], "fewer": [92, 111], "trainable": [93, 112], "parameters": [94], "than": [95], "existing": [96], "methods.": [97], "For": [98], "example,": [99], "our": [100], "model": [101], "outperforms": [102], "Flamingo80B": [103], "by": [104], "8.7%": [105], "zero-shot": [107, 122], "VQAv2": [108], "54x": [110], "parameters.": [113], "We": [114], "also": [115], "demonstrate": [116], "model's": [118], "emerging": [119], "capabilities": [120], "image-to-text": [123], "generation": [124], "can": [126], "follow": [127], "natural": [128], "instructions.": [130]}