{"Transformers": [0, 104], "have": [1], "a": [2, 12, 23, 33, 43, 48, 69], "potential": [3], "of": [4, 18, 42, 113, 153], "learning": [5, 30], "longer-term": [6, 59], "dependency,": [7, 60], "but": [8, 61], "are": [9, 161], "limited": [10], "by": [11], "fixed-length": [13], "context": [14, 65], "in": [15, 163], "the": [16, 64, 110], "setting": [17], "language": [19], "modeling.": [20], "We": [21], "propose": [22], "novel": [24, 49, 148], "neural": [25], "architecture": [26], "Transformer-XL": [27, 71, 142], "that": [28, 74], "enables": [29, 57], "dependency": [31, 73], "beyond": [32], "fixed": [34], "length": [35], "without": [36], "disrupting": [37], "temporal": [38], "coherence.": [39], "It": [40], "consists": [41], "segment-level": [44], "recurrence": [45], "mechanism": [46], "and": [47, 80, 92, 95, 130, 159, 166], "positional": [50], "encoding": [51], "scheme.": [52], "Our": [53, 155], "method": [54], "not": [55], "only": [56, 139], "capturing": [58], "also": [62], "resolves": [63], "fragmentation": [66], "problem.": [67], "As": [68], "result,": [70], "learns": [72], "is": [75, 96], "80%": [76], "longer": [77, 82], "than": [78, 83, 102], "RNNs": [79], "450%": [81], "vanilla": [84, 103], "Transformers,": [85], "achieves": [86], "better": [87], "performance": [88], "on": [89, 117, 120, 123, 126, 132, 140], "both": [90, 164], "short": [91], "long": [93], "sequences,": [94], "up": [97], "to": [98, 115, 144], "1,800+": [99], "times": [100], "faster": [101], "during": [105], "evaluation.": [106], "Notably,": [107], "we": [108], "improve": [109], "state-of-the-art": [111], "results": [112], "bpc/perplexity": [114], "0.99": [116], "enwiki8,": [118], "1.08": [119], "text8,": [121], "18.3": [122], "WikiText-103,": [124, 141], "21.8": [125], "One": [127], "Billion": [128], "Word,": [129], "54.5": [131], "Penn": [133], "Treebank": [134], "(without": [135], "finetuning).": [136], "When": [137], "trained": [138], "manages": [143], "generate": [145], "reasonably": [146], "coherent,": [147], "text": [149], "articles": [150], "with": [151], "thousands": [152], "tokens.": [154], "code,": [156], "pretrained": [157], "models,": [158], "hyperparameters": [160], "available": [162], "Tensorflow": [165], "PyTorch.": [167]}