{"We": [0], "present": [1], "Imagen,": [2], "a": [3, 14, 98, 142], "text-to-image": [4, 134, 148], "diffusion": [5, 39, 94], "model": [6, 75], "with": [7, 124, 155], "an": [8, 191], "unprecedented": [9], "degree": [10], "of": [11, 17, 25, 38, 72, 91, 103, 182, 193], "photorealism": [12], "and": [13, 33, 82, 114, 144, 163, 166, 185], "deep": [15], "level": [16], "language": [18, 28, 52, 74], "understanding.": [19], "Imagen": [20, 77, 96, 118, 154, 172], "builds": [21], "on": [22, 35, 57, 105, 112, 122], "the": [23, 36, 70, 73, 89, 92, 106, 125, 194], "power": [24], "large": [26, 51], "transformer": [27], "models": [29, 40, 53, 135, 175], "in": [30, 41, 76, 129, 136, 176, 180], "understanding": [31], "text": [32, 65], "hinges": [34], "strength": [37], "high-fidelity": [42], "image": [43, 67, 93], "generation.": [44], "Our": [45], "key": [46], "discovery": [47], "is": [48], "that": [49, 168], "generic": [50], "(e.g.": [54], "T5),": [55], "pretrained": [56], "text-only": [58], "corpora,": [59], "are": [60], "surprisingly": [61], "effective": [62], "at": [63], "encoding": [64], "for": [66, 147, 190], "synthesis:": [68], "increasing": [69, 88], "size": [71, 90], "boosts": [78], "both": [79, 179], "sample": [80, 183], "fidelity": [81], "image-text": [83, 130, 186], "alignment": [84], "much": [85], "more": [86], "than": [87], "model.": [95], "achieves": [97], "new": [99], "state-of-the-art": [100], "FID": [101], "score": [102], "7.27": [104], "COCO": [107, 126], "dataset,": [108], "without": [109], "ever": [110], "training": [111], "COCO,": [113], "human": [115, 169], "raters": [116, 170], "find": [117, 167], "samples": [119], "to": [120], "be": [121], "par": [123], "data": [127], "itself": [128], "alignment.": [131, 187], "To": [132], "assess": [133], "greater": [137], "depth,": [138], "we": [139, 152], "introduce": [140], "DrawBench,": [141, 151], "comprehensive": [143], "challenging": [145], "benchmark": [146], "models.": [149], "With": [150], "compare": [153], "recent": [156], "methods": [157], "including": [158], "VQ-GAN+CLIP,": [159], "Latent": [160], "Diffusion": [161], "Models,": [162], "DALL-E": [164], "2,": [165], "prefer": [171], "over": [173], "other": [174], "side-by-side": [177], "comparisons,": [178], "terms": [181], "quality": [184], "See": [188], "https://imagen.research.google/": [189], "overview": [192], "results.": [195]}