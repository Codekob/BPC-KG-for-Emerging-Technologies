{"Instruction": [0], "tuning": [1, 47, 138], "large": [2, 63], "language": [3, 77], "models": [4], "(LLMs)": [5], "using": [6], "machine-generated": [7], "instruction-following": [8, 43, 112], "data": [9], "has": [10], "improved": [11], "zero-shot": [12], "capabilities": [13], "on": [14, 48, 96, 108, 116], "new": [15, 127], "tasks,": [16], "but": [17], "the": [18, 24, 32, 91, 119], "idea": [19], "is": [20], "less": [21], "explored": [22], "in": [23], "multimodal": [25, 41, 64, 94, 111], "field.": [26], "In": [27], "this": [28], "paper,": [29], "we": [30, 52], "present": [31], "first": [33], "attempt": [34], "to": [35, 39], "use": [36], "language-only": [37], "GPT-4": [38, 95, 107, 124, 134], "generate": [40], "language-image": [42], "data.": [44], "By": [45], "instruction": [46, 137], "such": [49], "generated": [50, 135], "data,": [51, 139], "introduce": [53], "LLaVA:": [54], "Large": [55], "Language": [56], "and": [57, 71, 76, 99, 123, 142], "Vision": [58], "Assistant,": [59], "an": [60], "end-to-end": [61], "trained": [62], "model": [65, 141], "that": [66, 82], "connects": [67], "a": [68, 101, 109, 126], "vision": [69], "encoder": [70], "LLM": [72], "for": [73], "general-purpose": [74], "visual": [75, 136], "understanding.Our": [78], "early": [79], "experiments": [80], "show": [81], "LLaVA": [83, 122], "demonstrates": [84], "impressive": [85], "multimodel": [86], "chat": [87], "abilities,": [88], "sometimes": [89], "exhibiting": [90], "behaviors": [92], "of": [93, 121, 130], "unseen": [97], "images/instructions,": [98], "yields": [100], "85.1%": [102], "relative": [103], "score": [104], "compared": [105], "with": [106], "synthetic": [110], "dataset.": [113], "When": [114], "fine-tuned": [115], "Science": [117], "QA,": [118], "synergy": [120], "achieves": [125], "state-of-the-art": [128], "accuracy": [129], "92.53%.": [131], "We": [132], "make": [133], "our": [140], "code": [143], "base": [144], "publicly": [145], "available.": [146]}