{"We": [0, 107], "present": [1], "pure-transformer": [2], "based": [3, 137], "models": [4, 15, 69, 97], "for": [5], "video": [6, 118], "classification,": [7], "drawing": [8], "upon": [9], "the": [10, 25, 42, 60, 65, 89], "recent": [11], "success": [12], "of": [13, 35, 45, 55, 64], "such": [14], "in": [16, 48, 132], "image": [17, 96], "classification.": [18], "Our": [19], "model": [20, 57, 90], "extracts": [21], "spatiotemporal": [22], "tokens": [23, 46], "from": [24], "input": [26], "video,": [27, 49], "which": [28, 58], "are": [29, 70, 80], "then": [30], "encoded": [31], "by": [32], "a": [33], "series": [34], "transformer": [36], "layers.": [37], "In": [38], "order": [39], "to": [40, 72, 98, 101], "handle": [41], "long": [43], "sequences": [44], "encountered": [47], "we": [50, 82, 85], "propose": [51], "several,": [52], "efficient": [53], "variants": [54], "our": [56], "factorise": [59], "spatial-": [61], "and": [62, 93, 112, 124, 130], "temporal-dimensions": [63], "input.": [66], "Although": [67], "transformer-based": [68], "known": [71], "only": [73], "be": [74, 99], "effective": [75], "when": [76], "large": [77], "training": [78, 92], "datasets": [79], "available,": [81], "show": [83], "how": [84], "can": [86], "effectively": [87], "regularise": [88], "during": [91], "leverage": [94], "pretrained": [95], "able": [100], "train": [102], "on": [103, 116, 138], "comparatively": [104], "small": [105], "datasets.": [106], "conduct": [108], "thorough": [109], "ablation": [110], "studies,": [111], "achieve": [113], "state-of-the-art": [114], "results": [115], "multiple": [117], "classification": [119], "benchmarks": [120], "including": [121], "Kinetics": [122], "400": [123], "600,": [125], "Epic": [126], "Kitchens,": [127], "Something-Something": [128], "v2": [129], "Moments": [131], "Time,": [133], "outperforming": [134], "prior": [135], "methods": [136], "deep": [139], "3D": [140], "convolutional": [141], "networks.": [142]}