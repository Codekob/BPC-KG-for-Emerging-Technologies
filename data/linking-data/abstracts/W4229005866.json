{"Large": [0], "language": [1], "models,": [2], "which": [3, 76], "are": [4, 29, 40, 105], "often": [5], "trained": [6], "for": [7, 18, 119], "hundreds": [8], "of": [9, 11, 66, 123], "thousands": [10], "compute": [12], "days,": [13], "have": [14], "shown": [15], "remarkable": [16], "capabilities": [17], "zero-": [19], "and": [20, 81], "few-shot": [21], "learning.": [22], "Given": [23], "their": [24], "computational": [25], "cost,": [26], "these": [27], "models": [28], "difficult": [30, 55], "to": [31, 48, 56, 73, 79, 93, 102], "replicate": [32], "without": [33], "significant": [34], "capital.": [35], "For": [36], "the": [37, 49, 99, 111, 124], "few": [38], "that": [39, 89], "available": [41], "through": [42], "APIs,": [43], "no": [44], "access": [45], "is": [46, 91], "granted": [47], "full": [50], "model": [51], "weights,": [52], "making": [53], "them": [54], "study.": [57], "We": [58, 87, 104], "present": [59], "Open": [60], "Pre-trained": [61], "Transformers": [62], "(OPT),": [63], "a": [64], "suite": [65], "decoder-only": [67], "pre-trained": [68], "transformers": [69], "ranging": [70], "from": [71], "125M": [72], "175B": [74], "parameters,": [75], "we": [77, 114], "aim": [78], "fully": [80], "responsibly": [82], "share": [83], "with": [84, 117, 121], "interested": [85], "researchers.": [86], "show": [88], "OPT-175B": [90], "comparable": [92], "GPT-3,": [94], "while": [95], "requiring": [96], "only": [97], "1/7th": [98], "carbon": [100], "footprint": [101], "develop.": [103], "also": [106], "releasing": [107], "our": [108], "logbook": [109], "detailing": [110], "infrastructure": [112], "challenges": [113], "faced,": [115], "along": [116], "code": [118], "experimenting": [120], "all": [122], "released": [125], "models.": [126]}