{"Abstract": [0], "Large": [1], "language": [2], "models": [3, 24], "(LLMs)": [4], "have": [5], "demonstrated": [6], "impressive": [7], "capabilities,": [8], "but": [9, 197], "the": [10, 20, 154, 158, 219, 235], "bar": [11], "for": [12, 74, 181, 249], "clinical": [13, 21, 139, 250], "applications": [14], "is": [15], "high.": [16], "Attempts": [17], "to": [18, 35, 184, 200], "assess": [19], "knowledge": [22, 206], "of": [23, 62, 112, 157, 222, 231, 237], "typically": [25], "rely": [26], "on": [27, 31, 107, 119, 146], "automated": [28], "evaluations": [29, 228], "based": [30], "limited": [32], "benchmarks.": [33], "Here,": [34], "address": [36], "these": [37], "limitations,": [38], "we": [39, 90, 173], "present": [40], "MultiMedQA,": [41], "a": [42, 59, 70, 97, 110, 178, 188], "benchmark": [43], "combining": [44], "six": [45], "existing": [46], "medical": [47, 63], "question": [48], "answering": [49], "datasets": [50], "spanning": [51], "professional": [52], "medicine,": [53], "research": [54], "and": [55, 58, 86, 101, 132, 208, 214, 241], "consumer": [56], "queries": [57], "new": [60, 185], "dataset": [61, 123], "questions": [64], "searched": [65], "online,": [66], "HealthSearchQA.": [67], "We": [68, 202], "propose": [69], "human": [71, 165, 227], "evaluation": [72, 166, 239], "framework": [73], "model": [75, 212], "answers": [76], "along": [77], "multiple": [78], "axes": [79], "including": [80, 143], "factuality,": [81], "comprehension,": [82, 205], "reasoning,": [83], "possible": [84], "harm": [85], "bias.": [87], "In": [88], "addition,": [89], "evaluate": [91], "Pathways": [92], "Language": [93, 136], "Model": [94], "1": [95], "(PaLM,": [96], "540-billion": [98], "parameter": [99], "LLM)": [100], "its": [102], "instruction-tuned": [103], "variant,": [104], "Flan-PaLM": [105, 115], "2": [106], "MultiMedQA.": [108], "Using": [109], "combination": [111], "prompting": [113], "strategies,": [114], "achieves": [116], "state-of-the-art": [117], "accuracy": [118, 145], "every": [120], "MultiMedQA": [121], "multiple-choice": [122], "(MedQA": [124], "3": [125], ",": [126, 129], "MedMCQA": [127], "4": [128], "PubMedQA": [130], "5": [131], "Measuring": [133], "Massive": [134], "Multitask": [135], "Understanding": [137], "(MMLU)": [138], "topics": [140], "6": [141], "),": [142], "67.6%": [144], "MedQA": [147], "(US": [148], "Medical": [149], "Licensing": [150], "Exam-style": [151], "questions),": [152], "surpassing": [153], "prior": [155], "state": [156], "art": [159], "by": [160], "more": [161], "than": [162], "17%.": [163], "However,": [164], "reveals": [167], "key": [168], "gaps.": [169], "To": [170], "resolve": [171], "this,": [172], "introduce": [174], "instruction": [175, 215], "prompt": [176, 216], "tuning,": [177, 217], "parameter-efficient": [179], "approach": [180], "aligning": [182], "LLMs": [183, 223, 248], "domains": [186], "using": [187], "few": [189], "exemplars.": [190], "The": [191], "resulting": [192], "model,": [193], "Med-PaLM,": [194], "performs": [195], "encouragingly,": [196], "remains": [198], "inferior": [199], "clinicians.": [201], "show": [203], "that": [204], "recall": [207], "reasoning": [209], "improve": [210], "with": [211], "scale": [213], "suggesting": [218], "potential": [220], "utility": [221], "in": [224, 244], "medicine.": [225], "Our": [226], "reveal": [229], "limitations": [230], "today\u2019s": [232], "models,": [233], "reinforcing": [234], "importance": [236], "both": [238], "frameworks": [240], "method": [242], "development": [243], "creating": [245], "safe,": [246], "helpful": [247], "applications.": [251]}