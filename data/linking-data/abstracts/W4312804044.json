{"This": [0], "paper": [1], "presents": [2], "SimMIM,": [3], "a": [4, 39, 80, 89, 123, 159, 195], "simple": [5, 60], "framework": [6], "for": [7, 21], "masked": [8, 40, 83], "image": [9, 41, 78], "modeling.": [10], "We": [11, 179], "have": [12, 65], "simplified": [13], "recently": [14], "proposed": [15], "relevant": [16], "approaches,": [17], "without": [18], "the": [19, 50, 59, 76, 107, 115, 186], "need": [20], "special": [22], "designs,": [23], "such": [24], "as": [25, 120, 122], "block-wise": [26], "masking": [27, 74], "and": [28, 56], "tokenization": [29], "via": [30], "discrete": [31], "VAE": [32], "or": [33], "clustering.": [34], "To": [35], "investigate": [36], "what": [37], "makes": [38, 88], "modeling": [42], "task": [43], "learn": [44], "good": [45], "representations,": [46], "we": [47], "systematically": [48], "study": [49], "major": [51], "components": [52], "in": [53, 219], "our": [54, 135], "framework,": [55], "find": [57], "that": [58, 194, 218], "designs": [61], "of": [62, 75, 97], "each": [63], "component": [64], "revealed": [66], "very": [67], "strong": [68], "representation": [69], "learning": [70], "performance:": [71], "1)": [72], "random": [73], "input": [77], "with": [79, 111, 126, 162], "moderately": [81], "large": [82], "patch": [84, 108], "size": [85], "(e.g.,": [86], "32)": [87], "powerful": [90], "pre-text": [91], "task;": [92], "2)": [93], "predicting": [94], "RGB": [95], "values": [96], "raw": [98], "pixels": [99], "by": [100, 144, 154, 190], "direct": [101], "regression": [102], "performs": [103], "no": [104, 127], "worse": [105, 128], "than": [106, 130, 217], "classification": [109], "approaches": [110], "complex": [112], "designs;": [113], "3)": [114], "prediction": [116], "head": [117], "can": [118], "be": [119], "light": [121], "linear": [124], "layer,": [125], "performance": [129], "heavier": [131], "ones.": [132], "Using": [133], "ViT-B,": [134], "approach": [136, 153, 183], "achieves": [137, 169], "83.8%": [138], "top-1": [139, 171], "fine-tuning": [140], "accuracy": [141, 172, 206], "on": [142, 147, 173, 207], "ImageNet-1K": [143, 174, 177], "pre-training": [145], "also": [146, 180], "this": [148, 182], "dataset,": [149], "surpassing": [150], "previous": [151, 220], "best": [152], "+0.6%.": [155], "When": [156], "applied": [157], "to": [158, 184, 203], "larger": [160], "model": [161, 192, 197], "about": [163], "650": [164], "million": [165], "parameters,": [166], "SwinV2-H,": [167], "it": [168], "87.1%": [170], "using": [175, 212], "only": [176], "data.": [178], "leverage": [181], "address": [185], "data-hungry": [187], "issue": [188], "faced": [189], "large-scale": [191], "training,": [193], "3B": [196], "(Swin": [198], "V2-G)": [199], "is": [200, 225], "successfully": [201], "trained": [202], "achieve": [204], "state-of-the-art": [205], "four": [208], "representative": [209], "vision": [210], "benchmarks": [211], "40\u00d7": [213], "less": [214], "labelled": [215], "data": [216], "practice": [221], "(JFT-3B).": [222], "The": [223], "code": [224], "available": [226], "at": [227], "https://github.com/microsoft/SimMIM.": [228]}