{"Biomedical": [0, 103], "text": [1, 44, 56, 138, 167], "mining": [2, 45, 57, 139, 168], "is": [3, 107], "becoming": [4], "increasingly": [5], "important": [6], "as": [7], "the": [8, 16, 39, 50, 81, 120, 162, 211, 221], "number": [9], "of": [10, 41, 136, 153, 214], "biomedical": [11, 27, 43, 55, 73, 92, 116, 137, 144, 166, 170, 178, 186, 200, 207], "documents": [12], "rapidly": [13], "grows.": [14], "With": [15, 118], "progress": [17], "in": [18, 52, 133], "natural": [19], "language": [20, 85, 110], "processing": [21], "(NLP),": [22], "extracting": [23], "valuable": [24], "information": [25], "from": [26, 68, 100], "literature": [28], "has": [29, 37], "gained": [30], "popularity": [31], "among": [32], "researchers,": [33], "and": [34, 129, 185, 220], "deep": [35], "learning": [36], "boosted": [38], "development": [40], "effective": [42], "models.": [46], "However,": [47], "directly": [48], "applying": [49], "advancements": [51], "NLP": [53], "to": [54, 63, 72, 151, 204], "often": [58], "yields": [59], "unsatisfactory": [60], "results": [61, 194], "due": [62], "a": [64, 108, 134], "word": [65], "distribution": [66], "shift": [67], "general": [69], "domain": [70], "corpora": [71, 201], "corpora.": [74, 93, 117, 145], "In": [75], "this": [76], "article,": [77], "we": [78], "investigate": [79], "how": [80], "recently": [82], "introduced": [83], "pre-trained": [84, 113, 142, 212], "model": [86, 112], "BERT": [87, 128, 147, 198], "can": [88], "be": [89], "adapted": [90], "for": [91, 102, 224], "We": [94, 209], "introduce": [95], "BioBERT": [96, 125, 157, 215, 226], "(Bidirectional": [97], "Encoder": [98], "Representations": [99], "Transformers": [101], "Text": [104], "Mining),": [105], "which": [106], "domain-specific": [109], "representation": [111], "on": [114, 143, 161, 199], "large-scale": [115], "almost": [119], "same": [121], "architecture": [122], "across": [123], "tasks,": [124], "largely": [126], "outperforms": [127, 159], "previous": [130, 154], "state-of-the-art": [131, 155], "models": [132], "variety": [135], "tasks": [140], "when": [141], "While": [146], "obtains": [148], "performance": [149], "comparable": [150], "that": [152, 196], "models,": [156], "significantly": [158], "them": [160], "following": [163], "three": [164], "representative": [165], "tasks:": [169], "named": [171], "entity": [172], "recognition": [173], "(0.62%": [174], "F1": [175, 182], "score": [176, 183], "improvement),": [177], "relation": [179], "extraction": [180], "(2.80%": [181], "improvement)": [184], "question": [187], "answering": [188], "(12.24%": [189], "MRR": [190], "improvement).": [191], "Our": [192], "analysis": [193], "show": [195], "pre-training": [197], "helps": [202], "it": [203], "understand": [205], "complex": [206], "texts.": [208], "make": [210], "weights": [213], "freely": [216], "available": [217, 227], "at": [218, 228], "https://github.com/naver/biobert-pretrained,": [219], "source": [222], "code": [223], "fine-tuning": [225], "https://github.com/dmis-lab/biobert.": [229]}