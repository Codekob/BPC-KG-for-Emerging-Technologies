{"Recent": [0], "work": [1], "has": [2], "demonstrated": [3], "substantial": [4], "gains": [5], "on": [6, 14, 23, 156, 221], "many": [7, 157], "NLP": [8, 70, 158], "tasks": [9, 140, 170], "and": [10, 119, 141, 163, 255], "benchmarks": [11], "by": [12, 21, 245], "pre-training": [13], "a": [15, 24, 53, 59, 183, 187], "large": [16, 222], "corpus": [17], "of": [18, 39, 43, 45, 233, 252, 256], "text": [19, 147], "followed": [20], "fine-tuning": [22, 37, 97], "specific": [25], "task.": [26], "While": [27], "typically": [28], "task-agnostic": [29], "in": [30, 123, 186, 258], "architecture,": [31], "this": [32, 253], "method": [33], "still": [34, 72, 206], "requires": [35], "task-specific": [36], "datasets": [38, 201, 212], "thousands": [40, 44], "or": [41, 62, 137, 175, 189], "tens": [42], "examples.": [46], "By": [47], "contrast,": [48], "humans": [49], "can": [50, 230], "generally": [51], "perform": [52], "new": [54], "language": [55, 83, 105, 117], "task": [56], "from": [57, 63, 242], "only": [58], "few": [60], "examples": [61], "simple": [64], "instructions": [65], "-": [66], "something": [67], "which": [68, 236], "current": [69], "systems": [71], "largely": [73], "struggle": [74], "to": [75, 219], "do.": [76], "Here": [77], "we": [78, 100, 197, 226], "show": [79], "that": [80, 171, 228], "scaling": [81], "up": [82], "models": [84], "greatly": [85], "improves": [86], "task-agnostic,": [87], "few-shot": [88, 125, 142, 204], "performance,": [89], "sometimes": [90], "even": [91], "reaching": [92], "competitiveness": [93], "with": [94, 107, 139, 149], "prior": [95], "state-of-the-art": [96], "approaches.": [98], "Specifically,": [99], "train": [101], "GPT-3,": [102], "an": [103], "autoregressive": [104], "model": [106], "175": [108], "billion": [109], "parameters,": [110], "10x": [111], "more": [112], "than": [113], "any": [114, 134], "previous": [115], "non-sparse": [116], "model,": [118], "test": [120], "its": [121], "performance": [122, 155], "the": [124, 150, 194], "setting.": [126], "For": [127], "all": [128], "tasks,": [129, 165], "GPT-3": [130, 152, 214, 229, 257], "is": [131], "applied": [132], "without": [133], "gradient": [135], "updates": [136], "fine-tuning,": [138], "demonstrations": [143], "specified": [144], "purely": [145], "via": [146], "interaction": [148], "model.": [151], "achieves": [153], "strong": [154], "datasets,": [159], "including": [160], "translation,": [161], "question-answering,": [162], "cloze": [164], "as": [166, 168, 179, 208, 210], "well": [167, 209], "several": [169], "require": [172], "on-the-fly": [173], "reasoning": [174], "domain": [176], "adaptation,": [177], "such": [178], "unscrambling": [180], "words,": [181], "using": [182], "novel": [184], "word": [185], "sentence,": [188], "performing": [190], "3-digit": [191], "arithmetic.": [192], "At": [193], "same": [195], "time,": [196], "also": [198], "identify": [199], "some": [200, 211], "where": [202, 213], "GPT-3's": [203], "learning": [205], "struggles,": [207], "faces": [215], "methodological": [216], "issues": [217], "related": [218], "training": [220], "web": [223], "corpora.": [224], "Finally,": [225], "find": [227], "generate": [231], "samples": [232], "news": [234], "articles": [235, 243], "human": [237], "evaluators": [238], "have": [239], "difficulty": [240], "distinguishing": [241], "written": [244], "humans.": [246], "We": [247], "discuss": [248], "broader": [249], "societal": [250], "impacts": [251], "finding": [254], "general.": [259]}