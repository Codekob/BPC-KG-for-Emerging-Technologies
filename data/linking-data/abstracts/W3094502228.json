{"While": [0], "the": [1, 6], "Transformer": [2, 99], "architecture": [3], "has": [4], "become": [5], "de-facto": [7], "standard": [8], "for": [9], "natural": [10], "language": [11], "processing": [12], "tasks,": [13], "its": [14], "applications": [15], "to": [16, 34, 64, 86, 105, 115], "computer": [17], "vision": [18], "remain": [19], "limited.": [20], "In": [21], "vision,": [22], "attention": [23], "is": [24, 55], "either": [25], "applied": [26, 62], "in": [27, 46], "conjunction": [28], "with": [29], "convolutional": [30, 39, 107], "networks,": [31], "or": [32, 89], "used": [33], "replace": [35], "certain": [36], "components": [37], "of": [38, 66, 82], "networks": [40, 108], "while": [41, 109], "keeping": [42], "their": [43], "overall": [44], "structure": [45], "place.": [47], "We": [48], "show": [49], "that": [50], "this": [51], "reliance": [52], "on": [53, 73, 79], "CNNs": [54], "not": [56], "necessary": [57], "and": [58, 84], "a": [59], "pure": [60], "transformer": [61], "directly": [63], "sequences": [65], "image": [67, 74, 91], "patches": [68], "can": [69], "perform": [70], "very": [71], "well": [72], "classification": [75], "tasks.": [76], "When": [77], "pre-trained": [78], "large": [80], "amounts": [81], "data": [83], "transferred": [85], "multiple": [87], "mid-sized": [88], "small": [90], "recognition": [92], "benchmarks": [93], "(ImageNet,": [94], "CIFAR-100,": [95], "VTAB,": [96], "etc.),": [97], "Vision": [98], "(ViT)": [100], "attains": [101], "excellent": [102], "results": [103], "compared": [104], "state-of-the-art": [106], "requiring": [110], "substantially": [111], "fewer": [112], "computational": [113], "resources": [114], "train.": [116]}