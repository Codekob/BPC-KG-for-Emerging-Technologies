{"Pretrained": [0], "large": [1, 163, 177], "language": [2, 13], "models": [3], "(LLMs)": [4], "are": [5, 69, 82], "widely": [6], "used": [7], "in": [8, 48], "many": [9], "sub-fields": [10], "of": [11, 28, 172, 184, 201, 238], "natural": [12], "processing": [14], "(NLP)": [15], "and": [16, 50, 131, 156, 196, 241], "generally": [17], "known": [18], "as": [19, 167, 169, 222], "excellent": [20], "few-shot": [21, 76, 144, 255], "learners": [22], "with": [23, 162, 174], "task-specific": [24], "exemplars.": [25, 256], "Notably,": [26], "chain": [27], "thought": [29], "(CoT)": [30], "prompting,": [31], "a": [32], "recent": [33], "technique": [34], "for": [35, 64, 75, 228], "eliciting": [36], "complex": [37], "multi-step": [38], "reasoning": [39, 117, 126, 134, 191, 231], "through": [40], "step-by-step": [41], "answer": [42], "examples,": [43, 145], "achieved": [44], "the": [45, 60, 104, 148, 223, 229, 236, 243], "state-of-the-art": [46], "performances": [47, 113], "arithmetics": [49, 120], "symbolic": [51, 125], "reasoning,": [52], "difficult": [53], "system-2": [54], "tasks": [55, 118, 135, 192], "that": [56, 80, 100], "do": [57], "not": [58, 219], "follow": [59], "standard": [61], "scaling": [62], "laws": [63], "LLMs.": [65], "While": [66], "these": [67], "successes": [68], "often": [70], "attributed": [71], "to": [72, 154, 160], "LLMs'": [73], "ability": [74], "learning,": [77], "we": [78], "show": [79], "LLMs": [81, 249], "decent": [83], "zero-shot": [84, 111, 199, 226, 245], "reasoners": [85], "by": [86, 92, 212], "simply": [87], "adding": [88], "\"Let's": [89], "think": [90], "step": [91], "step\"": [93], "before": [94, 250], "each": [95], "answer.": [96], "Experimental": [97], "results": [98], "demonstrate": [99], "our": [101, 217], "Zero-shot-CoT,": [102], "using": [103], "same": [105], "single": [106, 186], "prompt": [107, 187], "template,": [108], "significantly": [109], "outperforms": [110], "LLM": [112], "on": [114, 150], "diverse": [115, 190], "benchmark": [116], "including": [119], "(MultiArith,": [121], "GSM8K,": [122], "AQUA-RAT,": [123], "SVAMP),": [124], "(Last": [127], "Letter,": [128], "Coin": [129], "Flip),": [130], "other": [132], "logical": [133], "(Date": [136], "Understanding,": [137], "Tracking": [138], "Shuffled": [139], "Objects),": [140], "without": [141], "any": [142], "hand-crafted": [143], "e.g.": [146], "increasing": [147], "accuracy": [149], "MultiArith": [151], "from": [152, 158], "17.7%": [153], "78.7%": [155], "GSM8K": [157], "10.4%": [159], "40.7%": [161], "InstructGPT": [164], "model": [165], "(text-davinci-002),": [166], "well": [168], "similar": [170], "magnitudes": [171], "improvements": [173], "another": [175], "off-the-shelf": [176], "model,": [178], "540B": [179], "parameter": [180], "PaLM.": [181], "The": [182], "versatility": [183], "this": [185], "across": [188], "very": [189], "hints": [193], "at": [194], "untapped": [195], "understudied": [197], "fundamental": [198], "capabilities": [200, 208], "LLMs,": [202], "suggesting": [203], "high-level,": [204], "multi-task": [205], "broad": [206], "cognitive": [207], "may": [209], "be": [210], "extracted": [211], "simple": [213], "prompting.": [214], "We": [215], "hope": [216], "work": [218], "only": [220], "serves": [221], "minimal": [224], "strongest": [225], "baseline": [227], "challenging": [230], "benchmarks,": [232], "but": [233], "also": [234], "highlights": [235], "importance": [237], "carefully": [239], "exploring": [240], "analyzing": [242], "enormous": [244], "knowledge": [246], "hidden": [247], "inside": [248], "crafting": [251], "finetuning": [252], "datasets": [253], "or": [254]}