{"Masked": [0], "language": [1, 210], "modeling": [2], "(MLM)": [3], "pre-training": [4, 57, 127], "methods": [5], "such": [6], "as": [7], "BERT": [8, 169], "corrupt": [9], "the": [10, 25, 66, 93, 97, 111, 135, 146, 156, 165, 171, 207, 241], "input": [11, 113, 141], "by": [12, 72, 116, 160, 168], "replacing": [13, 73], "some": [14, 74], "tokens": [15, 75, 142], "with": [16, 76], "[MASK]": [17], "and": [18, 176, 226, 236], "then": [19], "train": [20, 101, 189], "a": [21, 54, 81, 89, 102, 117, 154, 190], "model": [22, 90, 104, 173, 191], "to": [23, 35, 46, 224], "reconstruct": [24], "original": [26, 94], "tokens.": [27], "While": [28], "they": [29, 39], "produce": [30], "good": [31], "results": [32], "when": [33, 239], "transferred": [34], "downstream": [36], "NLP": [37], "tasks,": [38], "generally": [40], "require": [41], "large": [42], "amounts": [43], "of": [44, 64, 87, 96, 233, 244], "compute": [45, 235], "be": [47], "effective.": [48], "As": [49, 153], "an": [50], "alternative,": [51], "we": [52, 100, 188], "propose": [53], "more": [55, 130, 204], "sample-efficient": [56], "task": [58, 128, 136], "called": [59], "replaced": [60, 115], "token": [61, 109], "detection.": [62], "Instead": [63], "masking": [65], "input,": [67], "our": [68, 161], "approach": [69, 162, 214], "corrupts": [70], "it": [71, 221], "plausible": [77], "alternatives": [78], "sampled": [79], "from": [80], "small": [82, 147, 184], "generator": [83, 118], "network.": [84], "Then,": [85], "instead": [86], "training": [88], "that": [91, 105, 149, 198], "predicts": [92, 106], "identities": [95], "corrupted": [98, 112], "tokens,": [99], "discriminative": [103], "whether": [107], "each": [108], "in": [110], "was": [114, 150], "sample": [119], "or": [120], "not.": [121], "Thorough": [122], "experiments": [123], "demonstrate": [124], "this": [125], "new": [126], "is": [129, 137], "efficient": [131], "than": [132, 144, 231], "MLM": [133], "because": [134], "defined": [138], "over": [139], "all": [140], "rather": [143], "just": [145], "subset": [148], "masked": [151], "out.": [152], "result,": [155], "contextual": [157], "representations": [158], "learned": [159, 167], "substantially": [163], "outperform": [164], "ones": [166], "given": [170], "same": [172, 242], "size,": [174], "data,": [175], "compute.": [177, 245], "The": [178], "gains": [179], "are": [180], "particularly": [181], "strong": [182], "for": [183, 186, 195], "models;": [185], "example,": [187], "on": [192, 206], "one": [193], "GPU": [194], "4": [196], "days": [197], "outperforms": [199, 237], "GPT": [200], "(trained": [201], "using": [202, 229, 240], "30x": [203], "compute)": [205], "GLUE": [208], "natural": [209], "understanding": [211], "benchmark.": [212], "Our": [213], "also": [215], "works": [216], "well": [217], "at": [218], "scale,": [219], "where": [220], "performs": [222], "comparably": [223], "RoBERTa": [225], "XLNet": [227], "while": [228], "less": [230], "1/4": [232], "their": [234], "them": [238], "amount": [243]}