{"Attention-based": [0], "neural": [1], "networks": [2], "such": [3], "as": [4], "the": [5, 43, 75, 84, 88, 101], "Vision": [6, 57], "Transformer": [7, 47], "(ViT)": [8], "have": [9, 50], "recently": [10], "attained": [11], "state-of-the-art": [12, 121], "results": [13], "on": [14, 122, 142], "many": [15], "computer": [16], "vision": [17], "benchmarks.": [18], "Scale": [19], "is": [20, 34, 54], "a": [21, 30, 35, 105, 110, 119], "primary": [22], "ingredient": [23], "in": [24], "attaining": [25], "excellent": [26], "results,": [27], "therefore,": [28], "understanding": [29], "model's": [31], "scaling": [32, 46], "properties": [33], "key": [36], "to": [37], "designing": [38], "future": [39], "generations": [40], "effectively.": [41], "While": [42], "laws": [44], "for": [45, 133, 136], "language": [48], "models": [49, 66], "been": [51], "studied,": [52], "it": [53], "unknown": [55], "how": [56], "Transformers": [58], "scale.": [59], "To": [60], "address": [61], "this,": [62], "we": [63, 86, 107], "scale": [64], "ViT": [65, 111], "and": [67, 71, 73, 81, 90, 97], "data,": [68, 80], "both": [69], "up": [70], "down,": [72], "characterize": [74], "relationships": [76], "between": [77], "error": [78], "rate,": [79], "compute.": [82], "Along": [83], "way,": [85], "refine": [87], "architecture": [89], "training": [91], "of": [92, 100, 124], "ViT,": [93], "reducing": [94], "memory": [95], "consumption": [96], "increasing": [98], "accuracy": [99, 141], "resulting": [102], "models.": [103], "As": [104], "result,": [106], "successfully": [108], "train": [109], "model": [112, 129], "with": [113, 144], "two": [114], "billion": [115], "parameters,": [116], "which": [117], "attains": [118], "new": [120], "ImageNet": [123, 143], "90.45%": [125], "top-1": [126, 140], "accuracy.": [127], "The": [128], "also": [130], "performs": [131], "well": [132], "few-shot": [134], "transfer,": [135], "example,": [137], "reaching": [138], "84.86%": [139], "only": [145], "10": [146], "examples": [147], "per": [148], "class.": [149]}