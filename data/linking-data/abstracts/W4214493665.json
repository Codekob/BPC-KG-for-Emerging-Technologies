{"We": [0, 94], "present": [1], "in": [2, 18, 167, 175], "this": [3, 103], "paper": [4], "a": [5, 41, 46, 52, 57, 146, 164], "new": [6, 47], "architecture,": [7], "named": [8], "Convolutional": [9], "vision": [10, 184], "Transformer": [11, 16, 54], "(CvT),": [12], "that": [13, 102, 160], "improves": [14], "Vision": [15, 110, 169], "(ViT)": [17], "performance": [19, 107, 124], "and": [20, 51, 77, 91, 112, 119, 135], "efficiency": [21], "by": [22, 97], "introducing": [23], "convolutions": [24], "into": [25], "ViT": [26, 72], "to": [27, 70, 137], "yield": [28], "the": [29, 71, 82, 152, 161, 179], "best": [30], "of": [31, 43, 65, 84, 149], "both": [32], "de-signs.": [33], "This": [34], "is": [35], "accomplished": [36], "through": [37], "two": [38], "primary": [39], "modifications:": [40], "hierarchy": [42], "Transformers": [44, 85, 111], "containing": [45], "convolutional": [48, 53, 58, 66], "token": [49], "embedding,": [50], "block": [55], "leveraging": [56], "projection.": [59], "These": [60], "changes": [61], "introduce": [62], "desirable": [63], "properties": [64], "neural": [67], "networks": [68], "(CNNs)": [69], "architecture": [73], "(i.e.": [74, 86], "shift,": [75], "scale,": [76], "distortion": [78], "invariance)": [79], "while": [80], "maintaining": [81], "merits": [83], "dynamic": [87], "attention,": [88], "global": [89], "context,": [90], "better": [92], "generalization).": [93], "validate": [95], "CvT": [96], "conducting": [98], "extensive": [99], "experiments,": [100], "showing": [101], "approach": [104], "achieves": [105], "state-of-the-art": [106], "over": [108], "other": [109], "ResNets": [113], "on": [114, 130, 141, 151], "ImageNet-1k,": [115], "with": [116], "fewer": [117], "parameters": [118], "lower": [120], "FLOPs.": [121], "In": [122], "addition,": [123], "gains": [125], "are": [126], "maintained": [127], "when": [128], "pretrained": [129], "larger": [131], "datasets": [132], "(e.g.": [133], "ImageNet-22k)": [134], "fine-tuned": [136], "downstream": [138], "tasks.": [139, 185], "Pretrained": [140], "ImageNet-22k,": [142], "our": [143, 157, 176], "CvT-W24": [144], "obtains": [145], "top-1": [147], "accuracy": [148], "87.7%": [150], "ImageNet-1k": [153], "val": [154], "set.": [155], "Finally,": [156], "results": [158], "show": [159], "positional": [162], "encoding,": [163], "crucial": [165], "component": [166], "existing": [168], "Transformers,": [170], "can": [171], "be": [172, 188], "safely": [173], "re-moved": [174], "model,": [177], "simplifying": [178], "design": [180], "for": [181], "higher": [182], "resolution": [183], "Code": [186], "will": [187], "released": [189], "at": [190], "https://github.com/microsoft/CvT.": [191]}