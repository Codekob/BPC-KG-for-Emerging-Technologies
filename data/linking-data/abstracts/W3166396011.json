{"State-of-the-art": [0], "computer": [1, 135], "vision": [2, 136], "systems": [3], "are": [4], "trained": [5, 205], "to": [6, 31, 77, 103, 118, 159, 194], "predict": [7], "a": [8, 45, 50, 85, 167], "fixed": [9], "set": [10], "of": [11, 18, 54, 63, 87, 115, 125, 151, 185, 197], "predetermined": [12], "object": [13, 153], "categories.": [14], "This": [15], "restricted": [16], "form": [17], "supervision": [19], "limits": [20], "their": [21], "generality": [22], "and": [23, 74, 148, 162, 211], "usability": [24], "since": [25], "additional": [26], "labeled": [27], "data": [28], "is": [29, 44, 71, 101, 163], "needed": [30], "specify": [32], "any": [33, 175, 196], "other": [34], "visual": [35, 106], "concept.": [36], "Learning": [37], "directly": [38], "from": [39, 82, 94], "raw": [40], "text": [41], "about": [42], "images": [43], "promising": [46], "alternative": [47], "which": [48, 65, 69], "leverages": [49], "much": [51], "broader": [52], "source": [53], "supervision.": [55], "We": [56, 121, 207], "demonstrate": [57], "that": [58], "the": [59, 95, 116, 123, 172, 183, 186, 198], "simple": [60], "pre-training": [61], "task": [62], "predicting": [64], "caption": [66], "goes": [67], "with": [68, 166], "image": [70, 80], "an": [72], "efficient": [73], "scalable": [75], "way": [76], "learn": [78], "SOTA": [79], "representations": [81], "scratch": [83], "on": [84, 130, 189], "dataset": [86, 176], "400": [88], "million": [89, 200], "(image,": [90], "text)": [91], "pairs": [92], "collected": [93], "internet.": [96], "After": [97], "pre-training,": [98], "natural": [99], "language": [100], "used": [102], "reference": [104], "learned": [105], "concepts": [107], "(or": [108], "describe": [109], "new": [110], "ones)": [111], "enabling": [112], "zero-shot": [113, 191], "transfer": [114], "model": [117, 156, 213], "downstream": [119], "tasks.": [120], "study": [122], "performance": [124], "this": [126], "approach": [127], "by": [128], "benchmarking": [129], "over": [131], "30": [132], "different": [133], "existing": [134], "datasets,": [137], "spanning": [138], "tasks": [139, 161], "such": [140], "as": [141], "OCR,": [142], "action": [143], "recognition": [144], "in": [145], "videos,": [146], "geo-localization,": [147], "many": [149], "types": [150], "fine-grained": [152], "classification.": [154], "The": [155], "transfers": [157], "non-trivially": [158], "most": [160], "often": [164], "competitive": [165], "fully": [168], "supervised": [169], "baseline": [170], "without": [171, 192], "need": [173], "for": [174], "specific": [177], "training.": [178], "For": [179], "instance,": [180], "we": [181], "match": [182], "accuracy": [184], "original": [187], "ResNet-50": [188], "ImageNet": [190], "needing": [193], "use": [195], "1.28": [199], "training": [201], "examples": [202], "it": [203], "was": [204], "on.": [206], "release": [208], "our": [209], "code": [210], "pre-trained": [212], "weights": [214], "at": [215], "https://github.com/OpenAI/CLIP.": [216]}