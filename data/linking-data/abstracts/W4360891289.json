{"Large": [0], "language": [1, 10], "models": [2, 162, 167], "(LLMs)": [3], "have": [4], "demonstrated": [5], "remarkable": [6], "capabilities": [7], "in": [8, 80, 133, 250], "natural": [9], "understanding": [11], "and": [12, 33, 77, 112, 124, 158, 229, 254, 264], "generation": [13], "across": [14], "various": [15], "domains,": [16], "including": [17], "medicine.": [18, 137], "We": [19, 84, 202], "present": [20], "a": [21, 26, 38, 68, 174, 190, 212, 236], "comprehensive": [22], "evaluation": [23], "of": [24, 61, 92, 106, 120, 130, 177, 207, 219, 240, 248, 262], "GPT-4,": [25, 142], "state-of-the-art": [27], "LLM,": [28], "on": [29, 88, 114, 152, 170], "medical": [30, 46, 171, 223, 237, 251], "competency": [31, 76], "examinations": [32], "benchmark": [34, 93], "datasets.": [35, 94], "GPT-4": [36, 182, 220, 249], "is": [37, 42, 129, 183], "general-purpose": [39, 161], "model": [40, 97, 115, 209], "that": [41, 141, 197, 215], "not": [43], "specialized": [44, 145], "for": [45, 65, 118, 245], "problems": [47], "through": [48, 211], "training": [49], "or": [50], "engineered": [51], "to": [52, 73, 102, 193, 221, 227, 260], "solve": [53], "clinical": [54, 75, 255], "tasks.": [55], "Our": [56, 138], "analysis": [57], "covers": [58], "two": [59], "sets": [60], "official": [62], "practice": [63], "materials": [64], "the": [66, 81, 89, 104, 149, 195, 205, 208, 217, 241], "USMLE,": [67], "three-step": [69], "examination": [70], "program": [71], "used": [72], "assess": [74], "grant": [78], "licensure": [79], "United": [82], "States.": [83], "also": [85, 203], "evaluate": [86], "performance": [87], "MultiMedQA": [90], "suite": [91], "Beyond": [95], "measuring": [96], "performance,": [98, 116], "experiments": [99], "were": [100], "conducted": [101], "investigate": [103], "influence": [105], "test": [107], "questions": [108], "containing": [109], "both": [110], "text": [111], "images": [113], "probe": [117], "memorization": [119], "content": [121], "during": [122], "training,": [123], "study": [125, 214], "probability": [126], "calibration,": [127], "which": [128], "critical": [131], "importance": [132], "high-stakes": [134], "applications": [135], "like": [136], "results": [139], "show": [140], "without": [143], "any": [144], "prompt": [146], "crafting,": [147], "exceeds": [148], "passing": [150], "score": [151], "USMLE": [153], "by": [154], "over": [155], "20": [156], "points": [157], "outperforms": [159], "earlier": [160], "(GPT-3.5)": [163], "as": [164, 166], "well": [165], "specifically": [168], "fine-tuned": [169], "knowledge": [172], "(Med-PaLM,": [173], "prompt-tuned": [175], "version": [176], "Flan-PaLM": [178], "540B).": [179], "In": [180], "addition,": [181], "significantly": [184], "better": [185], "calibrated": [186], "than": [187], "GPT-3.5,": [188], "demonstrating": [189], "much-improved": [191], "ability": [192, 218], "predict": [194], "likelihood": [196], "its": [198], "answers": [199], "are": [200, 243], "correct.": [201], "explore": [204], "behavior": [206], "qualitatively": [210], "case": [213], "shows": [216], "explain": [222], "reasoning,": [224], "personalize": [225], "explanations": [226], "students,": [228], "interactively": [230], "craft": [231], "new": [232], "counterfactual": [233], "scenarios": [234], "around": [235], "case.": [238], "Implications": [239], "findings": [242], "discussed": [244], "potential": [246], "uses": [247], "education,": [252], "assessment,": [253], "practice,": [256], "with": [257], "appropriate": [258], "attention": [259], "challenges": [261], "accuracy": [263], "safety.": [265]}