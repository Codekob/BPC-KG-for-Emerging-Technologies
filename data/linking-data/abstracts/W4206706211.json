{"Astounding": [0], "results": [1], "from": [2, 52], "Transformer": [3, 115, 127], "models": [4, 128], "on": [5, 108, 238], "natural": [6], "language": [7], "tasks": [8, 113, 166, 179], "have": [9, 103], "intrigued": [10], "the": [11, 70, 126, 130, 143, 215], "vision": [12, 20, 112, 132, 162, 196], "community": [13], "to": [14, 18, 43, 93, 105, 120, 139], "study": [15], "their": [16, 23, 61, 230], "application": [17], "computer": [19, 131], "problems.": [21], "Among": [22], "salient": [24], "benefits,": [25], "Transformers": [26, 55, 74, 146], "enable": [27], "modeling": [28], "long": [29], "dependencies": [30], "between": [31], "input": [32], "sequence": [33, 40], "elements": [34], "and": [35, 63, 83, 89, 98, 151, 174, 185, 202, 204, 211, 218, 229, 242], "support": [36], "parallel": [37], "processing": [38, 76, 87, 189], "of": [39, 73, 111, 125, 145, 159, 220, 226], "as": [41, 67], "compared": [42], "recurrent": [44], "networks": [45, 97], "e.g.,": [46], "Long": [47], "short-term": [48], "memory": [49], "(LSTM).": [50], "Different": [51], "convolutional": [53], "networks,": [54], "require": [56], "minimal": [57], "inductive": [58], "biases": [59], "for": [60], "design": [62, 72, 228], "are": [64], "naturally": [65], "suited": [66], "set-functions.": [68], "Furthermore,": [69], "straightforward": [71], "allows": [75], "multiple": [77], "modalities": [78], "(e.g.,": [79, 167, 180, 190, 197, 207], "images,": [80], "videos,": [81], "text": [82], "speech)": [84], "using": [85, 114], "similar": [86], "blocks": [88], "demonstrates": [90], "excellent": [91], "scalability": [92], "very": [94], "large": [95], "capacity": [96], "huge": [99], "datasets.": [100], "These": [101], "strengths": [102], "led": [104], "exciting": [106], "progress": [107], "a": [109, 122], "number": [110], "networks.": [116], "This": [117], "survey": [118], "aims": [119], "provide": [121, 235], "comprehensive": [123], "overview": [124], "in": [129, 161, 224], "discipline.": [133], "We": [134, 154, 213], "start": [135], "with": [136], "an": [137, 236], "introduction": [138], "fundamental": [140], "concepts": [141], "behind": [142], "success": [144], "i.e.,": [147], "self-attention,": [148], "large-scale": [149], "pre-training,": [150], "bidirectional": [152], "encoding.": [153], "then": [155], "cover": [156], "extensive": [157], "applications": [158], "transformers": [160], "including": [163], "popular": [164, 221], "recognition": [165], "image": [168, 198, 200], "classification,": [169], "object": [170], "detection,": [171], "action": [172], "recognition,": [173, 192], "segmentation),": [175], "generative": [176], "modeling,": [177], "multi-modal": [178], "visual-question": [181], "answering,": [182], "visual": [183, 186], "reasoning,": [184], "grounding),": [187], "video": [188, 193], "activity": [191], "forecasting),": [194], "low-level": [195], "super-resolution,": [199], "enhancement,": [201], "colorization)": [203], "3D": [205], "analysis": [206, 237], "point": [208], "cloud": [209], "classification": [210], "segmentation).": [212], "compare": [214], "respective": [216], "advantages": [217], "limitations": [219], "techniques": [222], "both": [223], "terms": [225], "architectural": [227], "experimental": [231], "value.": [232], "Finally,": [233], "we": [234], "open": [239], "research": [240], "directions": [241], "possible": [243], "future": [244], "works.": [245]}