{"We": [0], "report": [1], "the": [2, 50, 64, 128], "development": [3], "of": [4, 53, 80, 90, 106, 116, 130], "GPT-4,": [5], "a": [6, 42, 47, 58, 68, 103], "large-scale,": [7], "multimodal": [8], "model": [9, 60], "which": [10], "can": [11], "accept": [12], "image": [13], "and": [14, 17, 37, 82, 96], "text": [15, 19], "inputs": [16], "produce": [18], "outputs.": [20], "While": [21], "less": [22], "capable": [23], "than": [24, 126], "humans": [25], "in": [26, 67, 75], "many": [27], "real-world": [28], "scenarios,": [29], "GPT-4": [30, 56], "exhibits": [31], "human-level": [32], "performance": [33, 77, 118], "on": [34, 78, 120], "various": [35], "professional": [36], "academic": [38], "benchmarks,": [39], "including": [40], "passing": [41], "simulated": [43], "bar": [44], "exam": [45], "with": [46, 123], "score": [48], "around": [49], "top": [51], "10%": [52], "test": [54], "takers.": [55], "is": [57], "Transformer-based": [59], "pre-trained": [61], "to": [62, 84, 111], "predict": [63, 113], "next": [65], "token": [66], "document.": [69], "The": [70], "post-training": [71], "alignment": [72], "process": [73], "results": [74], "improved": [76], "measures": [79], "factuality": [81], "adherence": [83], "desired": [85], "behavior.": [86], "A": [87], "core": [88], "component": [89], "this": [91], "project": [92], "was": [93], "developing": [94], "infrastructure": [95], "optimization": [97], "methods": [98], "that": [99], "behave": [100], "predictably": [101], "across": [102], "wide": [104], "range": [105], "scales.": [107], "This": [108], "allowed": [109], "us": [110], "accurately": [112], "some": [114], "aspects": [115], "GPT-4's": [117], "based": [119], "models": [121], "trained": [122], "no": [124], "more": [125], "1/1,000th": [127], "compute": [129], "GPT-4.": [131]}