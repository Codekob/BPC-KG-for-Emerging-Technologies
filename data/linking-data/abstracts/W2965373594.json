{"Language": [0], "model": [1, 84, 90], "pretraining": [2, 50], "has": [3], "led": [4], "to": [5], "significant": [6, 37], "performance": [7, 81], "gains": [8], "but": [9], "careful": [10], "comparison": [11], "between": [12], "different": [13, 27], "approaches": [14], "is": [15, 18], "challenging.": [16], "Training": [17], "computationally": [19], "expensive,": [20], "often": [21], "done": [22], "on": [23, 39, 94], "private": [24], "datasets": [25], "of": [26, 48, 60, 82, 104, 115], "sizes,": [28], "and,": [29], "as": [30], "we": [31], "will": [32], "show,": [33], "hyperparameter": [34], "choices": [35], "have": [36], "impact": [38, 59], "the": [40, 58, 80, 102, 113], "final": [41], "results.": [42], "We": [43, 68, 119], "present": [44], "a": [45], "replication": [46], "study": [47], "BERT": [49, 71], "(Devlin": [51], "et": [52], "al.,": [53], "2019)": [54], "that": [55, 70], "carefully": [56], "measures": [57], "many": [61], "key": [62], "hyperparameters": [63], "and": [64, 75, 97, 109, 123], "training": [65], "data": [66], "size.": [67], "find": [69], "was": [72], "significantly": [73], "undertrained,": [74], "can": [76], "match": [77], "or": [78], "exceed": [79], "every": [83], "published": [85], "after": [86], "it.": [87], "Our": [88], "best": [89], "achieves": [91], "state-of-the-art": [92], "results": [93, 100], "GLUE,": [95], "RACE": [96], "SQuAD.": [98], "These": [99], "highlight": [101], "importance": [103], "previously": [105], "overlooked": [106], "design": [107], "choices,": [108], "raise": [110], "questions": [111], "about": [112], "source": [114], "recently": [116], "reported": [117], "improvements.": [118], "release": [120], "our": [121], "models": [122], "code.": [124]}