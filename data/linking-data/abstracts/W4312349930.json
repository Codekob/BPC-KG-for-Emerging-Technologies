{"We": [0, 76], "present": [1], "techniques": [2, 178], "for": [3], "scaling": [4, 27], "Swin": [5, 32, 189], "Transformer": [6, 33, 190], "[35]": [7], "up": [8, 22, 28], "to": [9, 23, 85, 93, 117, 132, 142, 156, 168, 196], "3": [10, 187], "billion": [11, 188], "parameters": [12], "and": [13, 30, 67, 82, 111, 140, 163, 179, 192], "making": [14], "it": [15, 166, 195], "capable": [16], "of": [17, 21, 79, 121, 159, 212], "training": [18, 80], "with": [19, 173], "images": [20, 139, 202], "1,536x1,536": [24], "resolution.": [25], "By": [26], "capacity": [29], "resolution,": [31], "sets": [34], "new": [35], "records": [36], "on": [37, 45, 57, 63, 71, 209], "four": [38], "representative": [39], "vision": [40, 123, 171, 198], "benchmarks:": [41], "84.0%": [42], "top-1": [43, 69], "accuracy": [44, 70, 208], "ImageNet-": [46], "V2": [47], "image": [48], "classification,": [49], "63.1": [50], "/": [51, 54], "54.4": [52], "box": [53], "mask": [55], "mAP": [56], "COCO": [58], "object": [59], "detection,": [60], "59.9": [61], "mIoU": [62], "ADE20K": [64], "semantic": [65], "segmentation,": [66], "86.8%": [68], "Kinetics-400": [72], "video": [73], "action": [74], "classification.": [75], "tackle": [77], "issues": [78], "instability,": [81], "study": [83], "how": [84], "effectively": [86, 133, 193], "transfer": [87, 134, 194], "models": [88, 135, 172], "pre-trained": [89, 136], "at": [90, 137, 217], "low": [91], "resolutions": [92], "higher": [94], "resolution": [95], "ones.": [96], "To": [97], "this": [98], "aim,": [99], "several": [100], "novel": [101], "technologies": [102], "are": [103], "proposed:": [104], "1)": [105], "a": [106, 112, 126, 185, 210], "residual": [107], "post": [108], "normalization": [109], "technique": [110, 131], "scaled": [113], "cosine": [114], "attention": [115], "approach": [116], "improve": [118], "the": [119, 206], "stability": [120], "large": [122, 170], "models;": [124], "2)": [125], "log-spaced": [127], "continuous": [128], "position": [129], "bias": [130], "low-resolution": [138], "windows": [141], "their": [143], "higher-resolution": [144], "counterparts.": [145], "In": [146], "addition,": [147], "we": [148, 182], "share": [149], "our": [150], "crucial": [151], "implementation": [152], "details": [153], "that": [154], "lead": [155], "significant": [157], "savings": [158], "GPU": [160], "memory": [161], "consumption": [162], "thus": [164], "make": [165], "feasi-ble": [167], "train": [169, 184], "regular": [174], "GPUs.": [175], "Using": [176], "these": [177], "self-supervised": [180], "pre-training,": [181], "suc-cessfully": [183], "strong": [186], "model": [191], "various": [197], "tasks": [199], "involving": [200], "high-resolution": [201], "or": [203], "windows,": [204], "achieving": [205], "state-of-the-art": [207], "variety": [211], "benchmarks.": [213], "Code": [214], "is": [215], "avail-able": [216], "https://github.com/microsoft/Swin-Transformer.": [218]}