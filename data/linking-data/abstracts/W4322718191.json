{"We": [0, 15, 65], "introduce": [1], "LLaMA,": [2], "a": [3], "collection": [4], "of": [5, 21], "foundation": [6], "language": [7], "models": [8, 18, 32, 69], "ranging": [9], "from": [10], "7B": [11], "to": [12, 29, 40, 70], "65B": [13], "parameters.": [14], "train": [16, 30], "our": [17, 68], "on": [19, 51], "trillions": [20], "tokens,": [22], "and": [23, 42, 54, 63], "show": [24], "that": [25], "it": [26], "is": [27, 56], "possible": [28], "state-of-the-art": [31], "using": [33], "publicly": [34], "available": [35], "datasets": [36], "exclusively,": [37], "without": [38], "resorting": [39], "proprietary": [41], "inaccessible": [43], "datasets.": [44], "In": [45], "particular,": [46], "LLaMA-13B": [47], "outperforms": [48], "GPT-3": [49], "(175B)": [50], "most": [52], "benchmarks,": [53], "LLaMA-65B": [55], "competitive": [57], "with": [58], "the": [59, 71], "best": [60], "models,": [61], "Chinchilla-70B": [62], "PaLM-540B.": [64], "release": [66], "all": [67], "research": [72], "community.": [73]}