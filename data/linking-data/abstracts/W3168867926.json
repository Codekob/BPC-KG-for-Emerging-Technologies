{"An": [0], "important": [1], "paradigm": [2], "of": [3, 8, 47, 78, 86, 104, 168, 178], "natural": [4], "language": [5, 159], "processing": [6], "consists": [7], "large-scale": [9], "pre-training": [10], "on": [11, 128, 165], "general": [12], "domain": [13], "data": [14], "and": [15, 69, 110, 132, 183, 187, 193], "adaptation": [16], "to": [17, 93], "particular": [18], "tasks": [19], "or": [20, 61, 121], "domains.": [21], "As": [22], "we": [23], "pre-train": [24], "larger": [25], "models,": [26, 49], "full": [27], "fine-tuning,": [28], "which": [29, 63, 162], "retrains": [30], "all": [31], "model": [32, 67, 126, 160, 188], "parameters,": [33, 53, 138], "becomes": [34], "less": [35], "feasible.": [36], "Using": [37], "GPT-3": [38, 94], "175B": [39, 52, 95], "as": [40], "an": [41, 153], "example": [42], "--": [43], "deploying": [44], "independent": [45], "instances": [46], "fine-tuned": [48, 96], "each": [50, 76], "with": [51, 97, 180], "is": [54], "prohibitively": [55], "expensive.": [56], "We": [57, 150, 170], "propose": [58], "Low-Rank": [59], "Adaptation,": [60], "LoRA,": [62], "freezes": [64], "the": [65, 79, 84, 102, 111, 166, 176], "pre-trained": [66], "weights": [68], "injects": [70], "trainable": [71, 87, 105, 137], "rank": [72], "decomposition": [73], "matrices": [74], "into": [75, 156], "layer": [77], "Transformer": [80], "architecture,": [81], "greatly": [82], "reducing": [83], "number": [85, 103], "parameters": [88, 106], "for": [89, 190], "downstream": [90], "tasks.": [91], "Compared": [92], "Adam,": [98], "LoRA": [99, 118, 179], "can": [100], "reduce": [101], "by": [107, 115], "10,000": [108], "times": [109], "GPU": [112], "memory": [113], "requirement": [114], "3": [116], "times.": [117], "performs": [119], "on-par": [120], "better": [122], "than": [123], "fine-tuning": [124], "in": [125, 158], "quality": [127], "RoBERTa,": [129, 191], "DeBERTa,": [130, 192], "GPT-2,": [131], "GPT-3,": [133], "despite": [134], "having": [135], "fewer": [136], "a": [139, 172], "higher": [140], "training": [141], "throughput,": [142], "and,": [143], "unlike": [144], "adapters,": [145], "no": [146], "additional": [147], "inference": [148], "latency.": [149], "also": [151], "provide": [152, 184], "empirical": [154], "investigation": [155], "rank-deficiency": [157], "adaptation,": [161], "sheds": [163], "light": [164], "efficacy": [167], "LoRA.": [169], "release": [171], "package": [173], "that": [174], "facilitates": [175], "integration": [177], "PyTorch": [181], "models": [182], "our": [185], "implementations": [186], "checkpoints": [189], "GPT-2": [194], "at": [195], "https://github.com/microsoft/LoRA.": [196]}