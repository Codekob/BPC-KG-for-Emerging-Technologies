{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Companies and descriptions with Crunchbase\n",
    "\n",
    "We have a list of the top 100 highest evaluated Deep Tech startups in Europe. We can now use the Crunchbase API to get infomration relevant for linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get permalink\n",
    "# read the CSV file and extract the second column as an array\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/company-data/european_deep_tech_list.csv', header=None)\n",
    "permalinks = df[1].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 1/99 fetched anybotics\n",
      "✓ 2/99 fetched ably\n",
      "✓ 3/99 fetched accelercomm\n",
      "✓ 4/99 fetched addionics-london-united-kingdom\n",
      "✓ 5/99 fetched aerospacelab\n",
      "✓ 6/99 fetched aleph-alpha\n",
      "✓ 7/99 fetched alice-bob\n",
      "✓ 8/99 fetched aqemia\n",
      "✓ 9/99 fetched automata\n",
      "✓ 10/99 fetched axelera-ai\n",
      "✓ 11/99 fetched blickfeld\n",
      "✓ 12/99 fetched charm-therapeutics\n",
      "✓ 13/99 fetched core-power-holdings-pte-ltd\n",
      "✓ 14/99 fetched cado-security\n",
      "✓ 15/99 fetched causalens\n",
      "✓ 16/99 fetched contunity\n",
      "✓ 17/99 fetched cervest\n",
      "✓ 18/99 fetched cloudnc\n",
      "✓ 19/99 fetched cognigy\n",
      "✓ 20/99 fetched cybersmart-2\n",
      "✓ 21/99 fetched descartes-underwriting\n",
      "✓ 22/99 fetched destinus\n",
      "✓ 23/99 fetched diabeloop\n",
      "✓ 24/99 fetched diogenx\n",
      "✓ 25/99 fetched e-space\n",
      "✓ 26/99 fetched eleqtron-3d70\n",
      "✓ 27/99 fetched emergence-therapeutics\n",
      "✓ 28/99 fetched envisics\n",
      "✓ 29/99 fetched evox-therapeutics\n",
      "✓ 30/99 fetched exotrail\n",
      "✓ 31/99 fetched fairbrics\n",
      "✓ 32/99 fetched fairmat-tech\n",
      "✓ 33/99 fetched flockcover\n",
      "✓ 34/99 fetched ganymed-robotics\n",
      "✓ 35/99 fetched geopura\n",
      "✓ 36/99 fetched gourmey\n",
      "✓ 37/99 fetched h2-green-steel\n",
      "✓ 38/99 fetched halodi-robotics\n",
      "✓ 39/99 fetched harbr\n",
      "✓ 40/99 fetched heart-aerospace\n",
      "✓ 41/99 fetched helsing\n",
      "✓ 42/99 fetched hystar-706a\n",
      "✓ 43/99 fetched iceye\n",
      "✓ 44/99 fetched iqm\n",
      "✓ 45/99 fetched inato\n",
      "✓ 46/99 fetched isar-aerospace\n",
      "✓ 47/99 fetched kaia-health\n",
      "✓ 48/99 fetched kili-technology\n",
      "✓ 49/99 fetched lifebit-ai\n",
      "✓ 50/99 fetched mablink-bioscience\n",
      "✓ 51/99 fetched manna-16b7\n",
      "✓ 52/99 fetched marvel-fusion\n",
      "✓ 53/99 fetched meatable\n",
      "✓ 54/99 fetched mastor\n",
      "✓ 55/99 fetched morrow-batteries\n",
      "✓ 56/99 fetched mosa-meat\n",
      "✓ 57/99 fetched mostly-ai\n",
      "✓ 58/99 fetched nomagic\n",
      "✓ 59/99 fetched cb2tech\n",
      "✓ 60/99 fetched oni\n",
      "✓ 61/99 fetched ochre-bio\n",
      "✓ 62/99 fetched orbex-space\n",
      "✓ 63/99 fetched dibotics\n",
      "✓ 64/99 fetched oxford-ionics\n",
      "✓ 65/99 fetched paragraf\n",
      "✓ 66/99 fetched pasqal\n",
      "✓ 67/99 fetched poly-ai\n",
      "✓ 68/99 fetched previse\n",
      "✓ 69/99 fetched pricehubble\n",
      "✓ 70/99 fetched proximie\n",
      "✓ 71/99 fetched quantinuum\n",
      "✓ 72/99 fetched quantum-motion-technologies\n",
      "✓ 73/99 fetched quantum-surgical\n",
      "✓ 74/99 fetched redsift\n",
      "✓ 75/99 fetched resolution-games\n",
      "✓ 76/99 fetched riverlane\n",
      "[warn] rossum : not found\n",
      "✓ 77/99 fetched rossum \n",
      "✓ 78/99 fetched sana-2\n",
      "✓ 79/99 fetched global-satellite-vu\n",
      "✓ 80/99 fetched secondmind\n",
      "✓ 81/99 fetched sipearl\n",
      "✓ 82/99 fetched solar-foods-oy\n",
      "✓ 83/99 fetched storegga-geotechnologies\n",
      "✓ 84/99 fetched sylvera\n",
      "✓ 85/99 fetched synthesia\n",
      "✓ 86/99 fetched twaice\n",
      "✓ 87/99 fetched the-exploration-company\n",
      "✓ 88/99 fetched the-plum-guide\n",
      "✓ 89/99 fetched treefrog-therapeutics\n",
      "✓ 90/99 fetched nan\n",
      "✓ 91/99 fetched nan\n",
      "✓ 92/99 fetched nan\n",
      "✓ 93/99 fetched nan\n",
      "✓ 94/99 fetched nan\n",
      "✓ 95/99 fetched nan\n",
      "✓ 96/99 fetched nan\n",
      "✓ 97/99 fetched nan\n",
      "✓ 98/99 fetched nan\n",
      "✓ 99/99 fetched nan\n",
      "Saved raw JSON data for 98 companies to ../data/company-data/companies_raw.json\n",
      "Fetched and saved data for 98 companies.\n",
      "Saved summary CSV for 98 companies to ../data/company-data/companies_summary.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Iterable, Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Configuration\n",
    "# ------------------------------------------------------------------\n",
    "load_dotenv()  # loads CRUNCHBASE_API_KEY from .env\n",
    "API_KEY        = os.getenv(\"CRUNCHBASE_API_KEY\")\n",
    "BASE_URL       = \"https://api.crunchbase.com/api/v4/entities/organizations\"\n",
    "# Fields to fetch\n",
    "DEFAULT_FIELDS = [\n",
    "    \"identifier\",            # permalink, uuid, name, etc.\n",
    "    \"short_description\",\n",
    "    \"description\",\n",
    "    \"website\",\n",
    "    \"company_type\",\n",
    "    \"location_identifiers\",\n",
    "    \"funding_total\",\n",
    "    \"founded_on\",\n",
    "    \"funding_stage\",\n",
    "    \"category_groups\",\n",
    "    \"categories\",\n",
    "]\n",
    "\n",
    "# polite crawl settings (v4 limit is 50 req/min = 1.2 s / call)\n",
    "REQUEST_INTERVAL = 1.3  # seconds\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Helper to call the API\n",
    "# ------------------------------------------------------------------\n",
    "def _get_organization(permalink: str,\n",
    "                      field_ids: Iterable[str] | None = None,\n",
    "                      api_key: str | None = None) -> Dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    Fetch one organization entity. Returns None on 404 or errors.\n",
    "    \"\"\"\n",
    "    api_key   = api_key or API_KEY\n",
    "    field_str = \",\".join(field_ids or DEFAULT_FIELDS)\n",
    "    url       = f\"{BASE_URL}/{permalink}\"\n",
    "    params    = {\"user_key\": api_key, \"field_ids\": field_str}\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=15)\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get(\"properties\", {})\n",
    "        elif r.status_code == 404:\n",
    "            print(f\"[warn] {permalink}: not found\")\n",
    "        else:\n",
    "            print(f\"[warn] {permalink}: HTTP {r.status_code} – {r.text}\")\n",
    "    except requests.RequestException as exc:\n",
    "        print(f\"[error] {permalink}: {exc}\")\n",
    "    return None\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Public wrapper for a list of permalinks\n",
    "# ------------------------------------------------------------------\n",
    "def fetch_companies(permalinks: Iterable[str],\n",
    "                    field_ids: Iterable[str] | None = None,\n",
    "                    sleep: float = REQUEST_INTERVAL,\n",
    "                    api_key: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch many companies, save raw JSON array, and return a DataFrame.\n",
    "    \"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for i, pl in enumerate(permalinks, 1):\n",
    "        props = _get_organization(pl, field_ids, api_key)\n",
    "        if props is not None:\n",
    "            results.append(props)\n",
    "        time.sleep(sleep)\n",
    "        print(f\"✓ {i}/{len(permalinks)} fetched {pl}\")\n",
    "\n",
    "    # Save raw JSON array of company properties\n",
    "    json_output = '../data/company-data/companies_raw.json'\n",
    "    os.makedirs(os.path.dirname(json_output), exist_ok=True)\n",
    "    with open(json_output, 'w', encoding='utf-8') as jf:\n",
    "        json.dump(results, jf, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved raw JSON data for {len(results)} companies to {json_output}\")\n",
    "\n",
    "    # Flatten into DataFrame\n",
    "    df = pd.json_normalize(results)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Usage example\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Read permalinks from CSV\n",
    "    source_csv = '../data/company-data/european_deep_tech_list.csv'\n",
    "    df_input = pd.read_csv(source_csv, header=None)\n",
    "    permalinks = df_input[1].astype(str).tolist()\n",
    "\n",
    "    # Fetch data\n",
    "    df = fetch_companies(permalinks)\n",
    "    print(f\"Fetched and saved data for {len(df)} companies.\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5.  Extract and save to CSV\n",
    "    # ------------------------------------------------------------------\n",
    "    summary_csv = '../data/company-data/companies_summary.csv'\n",
    "    os.makedirs(os.path.dirname(summary_csv), exist_ok=True)\n",
    "\n",
    "    summary_df = df[['identifier.value', 'description', 'short_description']].copy()\n",
    "    summary_df.columns = ['value', 'description', 'short_description']\n",
    "    summary_df.to_csv(summary_csv, index=False, encoding='utf-8')\n",
    "    print(f\"Saved summary CSV for {len(summary_df)} companies to {summary_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
