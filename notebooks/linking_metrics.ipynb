{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9cd268",
   "metadata": {},
   "source": [
    "# Caluclating Metrics from our linking\n",
    "\n",
    "todo:\n",
    "calucalting simple metrics \n",
    "    - how many papers got linked to a technology\n",
    "    - how many papers per technology average\n",
    "    - how often one paper gets linked on average\n",
    "\n",
    "fixing abstract generation: \n",
    "    creata a list of the papers that havent been fetched yet, count that and then only fetch for them\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df623015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd05f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import linking data\n",
    "links_df = pd.read_csv('../data/linking-data/paper_technology_links.csv')\n",
    "links_df['paper_id'] = links_df['paper_id'].str.replace('https://openalex.org/', '')\n",
    "paper_ids = links_df['paper_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b463c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "url = \"https://api.openalex.org/works\"\n",
    "mail = os.getenv(\"MAIL\")\n",
    "\n",
    "\n",
    "\n",
    "for paper in paper_ids:\n",
    "    request_url = f\"{url}/{paper}\"\n",
    "    params = {\n",
    "        \"mailto\": mail,\n",
    "        \"select\": \"abstract_inverted_index\"\n",
    "    }\n",
    "    response = requests.get(request_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'abstract_inverted_index' in data:\n",
    "            abstract_inverted_index = data['abstract_inverted_index']\n",
    "            with open(f\"../data/linking-data/abstracts/{paper}.json\", \"w\") as f:\n",
    "                json.dump(abstract_inverted_index, f)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for paper {paper}: {response.status_code}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24581bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 5150\n",
      "Number of papers without abstract: 1649\n",
      "Number of papers after removing null abstracts: 3491\n"
     ]
    }
   ],
   "source": [
    "# how many papers without abstract \n",
    "import glob\n",
    "\n",
    "null_abstracts = 0\n",
    "for file in glob.glob(\"../data/linking-data/abstracts/*.json\"):\n",
    "    with open(file) as f:\n",
    "        content = json.load(f)\n",
    "        if content is None:\n",
    "            null_abstracts += 1\n",
    "            \n",
    "print(f\"Total number of papers: {len(links_df)}\")\n",
    "print(f\"Number of papers without abstract: {null_abstracts}\")\n",
    "\n",
    "# Remove papers with null abstracts from links_df\n",
    "papers_with_abstracts = []\n",
    "for file in glob.glob(\"../data/linking-data/abstracts/*.json\"):\n",
    "    with open(file) as f:\n",
    "        content = json.load(f)\n",
    "        if content is not None:\n",
    "            paper_id = os.path.basename(file).replace('.json', '')\n",
    "            papers_with_abstracts.append(paper_id)\n",
    "\n",
    "links_df = links_df[links_df['paper_id'].isin(papers_with_abstracts)]\n",
    "print(f\"Number of papers after removing null abstracts: {len(links_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afe740",
   "metadata": {},
   "source": [
    "There are 5150 papers that got linkek. 1649 do not have an abstract which we want to use to caluclate metrics for clustering. This will be a limitation for our metric. We will still calculate the Silhouette Score and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ec46b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5096/5096 [00:02<00:00, 1819.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert abstracts to text\n",
    "from tqdm import tqdm\n",
    "\n",
    "def inv_index(abstraced_index):\n",
    "    pos2word = {}\n",
    "    for w, ps in abstraced_index.items():\n",
    "            for p in ps:\n",
    "                pos2word[p] = w\n",
    "    return \" \".join(pos2word[i] for i in sorted(pos2word))\n",
    "\n",
    "# convert all abstracts to text\n",
    "\n",
    "abstracts_data = []\n",
    "for file in tqdm(glob.glob(\"../data/linking-data/abstracts/*.json\")):\n",
    "    with open(file) as f:\n",
    "        content = json.load(f)\n",
    "        if content is not None:\n",
    "            paper_id = os.path.basename(file).replace('.json', '')\n",
    "            abstract_text = inv_index(content)\n",
    "            abstracts_data.append([paper_id, abstract_text])\n",
    "\n",
    "abstracts_df = pd.DataFrame(abstracts_data, columns=['paper_id', 'abstract'])\n",
    "abstracts_df.to_csv('../data/linking-data/abstracts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6106e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add technologies to csv\n",
    "# merge abstracts with technologies\n",
    "abstracts_df = pd.read_csv('../data/linking-data/abstracts.csv')\n",
    "merged_df = pd.merge(abstracts_df, links_df, on='paper_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc7d19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2e2ba133aa4794b9019482cea1d0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate embeddings for each abstract\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "merged_df = merged_df[merged_df['abstract'].notnull()]\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "embeddings = model.encode(merged_df['abstract'].tolist(), show_progress_bar=True, batch_size=16)\n",
    "\n",
    "merged_df['tech_code'] = merged_df['technology_name'].astype('category').cat.codes\n",
    "labels = merged_df['tech_code'].tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
