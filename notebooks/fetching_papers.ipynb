{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process of Fetching Papers\n",
    "\n",
    "## Overview\n",
    "\n",
    "We need to fetch papers and their relevant metadata to later capture them as entities and link them to companies and technologies.\n",
    "\n",
    "We use OpenAlex API for this because it records various useful properties of research papers.\n",
    "\n",
    "We fetch the top 1 000 cited papers for each year 2024-2019 that have abstracts. We will fetch the following properties:\n",
    "\n",
    "1. OpenAlex ID\n",
    "2. DOI\n",
    "3. title\n",
    "4. publication_date\n",
    "5. cited_by_count\n",
    "6. authorships\n",
    "7. keywords\n",
    "8. topics \n",
    "9. abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 papers found\n",
      "Raw papers saved to ../data/papers-data/raw_papers_v2.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "#OpenAlex API endpoint\n",
    "url = \"https://api.openalex.org/works\"\n",
    "mail_to = os.getenv(\"MAIL\")\n",
    "\n",
    "years = [2024, 2023, 2022, 2021, 2020, 2019]\n",
    "all_papers = []\n",
    "\n",
    "per_page = 200\n",
    "\n",
    "for year in years:\n",
    "    paper_count = 0 \n",
    "    page = 1\n",
    "    while paper_count < 1000:\n",
    "        params = {\n",
    "            \"filter\": f\"publication_year:{year}\",\n",
    "            \"per-page\": 200,\n",
    "            \"mailto\": mail_to,\n",
    "            \"sort\": \"cited_by_count:desc\",\n",
    "            \"page\": page,\n",
    "            \"per-page\": per_page,\n",
    "            \"select\": \"id, doi, title, publication_date, authorships, cited_by_count, keywords, topics, abstract_inverted_index\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            papers = data.get('results', [])\n",
    "            if not papers:\n",
    "                break\n",
    "            \n",
    "            # Filter out papers with null abstract_inverted_index\n",
    "            papers_with_abstract = [p for p in papers if p.get('abstract_inverted_index') is not None]\n",
    "            remaining_needed = 1000 - paper_count\n",
    "            papers_to_add = papers_with_abstract[:remaining_needed]\n",
    "            paper_count += len(papers_to_add)\n",
    "            all_papers.extend(papers_to_add)\n",
    "            if paper_count >= 1000:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Error fetching data for year {year}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        \n",
    "print(len(all_papers), \"papers found\")\n",
    "\n",
    "raw_path = \"../data/papers-data/raw_papers_v2.jsonl\"\n",
    "with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in all_papers:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Raw papers saved to\", raw_path)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 000 papers, 1 000 from each year sorted by most cited. Each entry is a rather large JSON that contains a lot of unnecessary data for our use case. Now we clean and restrucutre the data into a useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def inv_index(abstract_index: Dict[str, List[int]]) -> str:\n",
    "    \"\"\"\n",
    "    Convert an inverted index of an abstract into its plain text form.\n",
    "\n",
    "    Args:\n",
    "        abstract_index: A mapping from words to lists of positions.\n",
    "\n",
    "    Returns:\n",
    "        The reconstructed abstract as a single string.\n",
    "    \"\"\"\n",
    "    pos2word: Dict[int, str] = {}\n",
    "    for word, positions in abstract_index.items():\n",
    "        for pos in positions:\n",
    "            pos2word[pos] = word\n",
    "    # Reassemble text in correct order\n",
    "    return \" \".join(pos2word[i] for i in sorted(pos2word))\n",
    "\n",
    "def clean_paper(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\" Return a cleaned copy of one paper\"\"\"\n",
    "    paper = deepcopy(paper)\n",
    "    \n",
    "    cleaned_authors: List[Dict[str, Any]] = []\n",
    "    institutions: Dict[str, Dict[str, Any]] = {}\n",
    "    \n",
    "    for author in paper.get(\"authorships\", []):\n",
    "        # slim author\n",
    "        cleaned_authors.append(\n",
    "            {\n",
    "                \"author_position\": author.get(\"author_position\"),\n",
    "                \"display_name\": author.get(\"author\", {}).get(\"display_name\"),\n",
    "                \"orcid\": author.get(\"author\", {}).get(\"orcid\"),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # collect institutions\n",
    "        for institution in author.get(\"institutions\", []):\n",
    "            # drop lineage field\n",
    "            institution_without_lineage = {k: v for k, v in institution.items() if k != \"lineage\"}\n",
    "            institutions[institution_without_lineage[\"id\"]] = institution_without_lineage\n",
    "    \n",
    "    paper[\"authorships\"] = cleaned_authors\n",
    "    paper[\"institutions\"] = list(institutions.values())\n",
    "    \n",
    "    # keywords\n",
    "    \n",
    "    paper[\"keywords\"] = [\n",
    "        {\n",
    "            \"id\": keywords.get(\"id\"),\n",
    "            \"display_name\": keywords.get(\"display_name\"),\n",
    "            \"score\": keywords.get(\"score\"),\n",
    "        }\n",
    "        for keywords in paper.get(\"keywords\", [])\n",
    "    ]\n",
    "    \n",
    "    # topics\n",
    "    \n",
    "    topics_raw = paper.get(\"topics\", [])\n",
    "    paper[\"topics\"] = [\n",
    "        {\n",
    "            \"display_name\": topic.get(\"display_name\"),\n",
    "            \"score\": topic.get(\"score\")\n",
    "        }\n",
    "            for topic in topics_raw\n",
    "    ]\n",
    "    \n",
    "    # helper dictionaires for deduplication\n",
    "    \n",
    "    subfields, fields, domains = {}, {}, {}\n",
    "    \n",
    "    \n",
    "    for topic in topics_raw:\n",
    "        subfield = topic.get(\"subfield\")\n",
    "        if subfield:\n",
    "            subfields[subfield[\"id\"]] = {\"id\": subfield[\"id\"], \"display_name\": subfield[\"display_name\"]}\n",
    "\n",
    "        field = topic.get(\"field\")\n",
    "        if field:\n",
    "            fields[field[\"id\"]] = {\"id\": field[\"id\"], \"display_name\": field[\"display_name\"]}\n",
    "\n",
    "        domain = topic.get(\"domain\")\n",
    "        if domain:\n",
    "            domains[domain[\"id\"]] = {\"id\": domain[\"id\"], \"display_name\": domain[\"display_name\"]}\n",
    "    \n",
    "    paper[\"subfields\"] = list(subfields.values())\n",
    "    paper[\"fields\"] = list(fields.values())\n",
    "    paper[\"domains\"] = list(domains.values())\n",
    "    \n",
    "    # Reconstruct abstract from inverted index, if present\n",
    "    abstract_index = paper.get(\"abstract_inverted_index\")\n",
    "    if isinstance(abstract_index, dict):\n",
    "        paper[\"abstract\"] = inv_index(abstract_index)\n",
    "        # Optionally remove the raw index\n",
    "        paper.pop(\"abstract_inverted_index\", None)\n",
    "\n",
    "    return paper\n",
    "\n",
    "def stream_jsonl(path: str):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "                \n",
    "def write_jsonl(records, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        for record in records:\n",
    "            fh.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"../data/papers-data/raw_papers_v2.jsonl\"\n",
    "cleaned_path = \"../data/papers-data/cleaned_papers_v2.jsonl\"\n",
    "\n",
    "cleaned_gen = (clean_paper(p) for p in stream_jsonl(raw_path))\n",
    "write_jsonl(cleaned_gen, cleaned_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extracted the relevant properties out of our API request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the structure of our JSON Data\n",
    "\n",
    "üîë Top-Level Fields\n",
    "Field\tType\tDescription\n",
    "id\tstring\tUnique identifier for the paper\n",
    "doi\tstring\tDOI of the paper\n",
    "title\tstring\tTitle of the paper\n",
    "publication_date\tstring\tDate of publication\n",
    "cited_by_count\tinteger\tNumber of times the paper has been cited\n",
    "\n",
    "üë®‚Äçüî¨ Authorships\n",
    "An array of author objects:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "\"authorships\": [\n",
    "  {\n",
    "    \"author_position\": \"first\" | \"middle\" | \"last\",\n",
    "    \"display_name\": \"Full name\",\n",
    "    \"orcid\": \"ORCID identifier\" or null\n",
    "  }\n",
    "]\n",
    "üè∑Ô∏è Keywords\n",
    "An array of keyword objects describing the paper:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "\"keywords\": [\n",
    "  {\n",
    "    \"id\": \"keyword_id\",\n",
    "    \"display_name\": \"Keyword name\",\n",
    "    \"score\": float\n",
    "  }\n",
    "]\n",
    "üìö Topics\n",
    "An array of general topics the paper is related to:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "\"topics\": [\n",
    "  {\n",
    "    \"display_name\": \"Topic name\",\n",
    "    \"score\": float\n",
    "  }\n",
    "]\n",
    "üèõÔ∏è Institutions\n",
    "An array of affiliated institutions:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "\"institutions\": [\n",
    "  {\n",
    "    \"id\": \"institution_id\",\n",
    "    \"display_name\": \"Institution name\",\n",
    "    \"ror\": \"ROR ID\",\n",
    "    \"country_code\": \"e.g. US, DE\",\n",
    "    \"type\": \"e.g. education, company\"\n",
    "  }\n",
    "]\n",
    "üß™ Classification Fields\n",
    "Each of these is an array of objects with id and display_name.\n",
    "\n",
    "Subfields (e.g. \"Machine Learning\", \"Biophysics\")\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "\"subfields\": [ { \"id\": \"...\", \"display_name\": \"...\" } ]\n",
    "Fields (e.g. \"Computer Science\", \"Biology\")\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "\"fields\": [ { \"id\": \"...\", \"display_name\": \"...\" } ]\n",
    "Domains (e.g. \"STEM\", \"Social Sciences\")\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "\"domains\": [ { \"id\": \"...\", \"display_name\": \"...\" } ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
