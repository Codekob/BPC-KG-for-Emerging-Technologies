{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process of fetching the Emerging Technologies\n",
    "\n",
    "## Overview \n",
    "\n",
    "1. Manually collecting a List of Emerging Technologies \n",
    "2. Enriching the technology names with a one-sentence definition\n",
    "3. Compute sentence embeddings\n",
    "4. Cluster embeddings\n",
    "5. Name clusters\n",
    "6. Manually review the clusters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Manually collect Technologies\n",
    "\n",
    "We manually collected emerging technologies using different reports from known industry entities and a list from Wikipedia to collect a general list of various emerging technologies. The list is called manual_selected_technologies.csv and can be found in the data/ directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Enrich the technologies\n",
    "\n",
    "The next step is to enrich the technolgies with their definition. We do that because we run an embedding model later on and with the definitions this model will yield better results. We will not add this script to this notebook because it is only a generic step but one can find the script for that in the scripts/enrich/ folder. We used gpt-4.1-mini because it has it's knowledge cutoff in 2024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute sentence embeddings\n",
    "\n",
    "We compute the embeddings of the terms using the term and the sentence. We chosose the all-MiniLM-L6-v2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221931e840444466ba2b047f22ab7b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "df = pd.read_csv('../data/technologies-data/technologies_with_definitions.csv')\n",
    "df[\"embed_text\"] = df[\"Technology Name\"].str.strip() + \". \" + df[\"Definition\"].str.strip()\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "embeddings = model.encode(\n",
    "    df[\"embed_text\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=16,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "np.save('../data/technologies-data/tech_embeddings.npy', embeddings)\n",
    "df.to_parquet(\"../data/technologies-data/tech_terms_enriched.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Clusters\n",
    "\n",
    "Now we create the clusters to cluster our Technologies. We use HDBSCAN because we do not know how many clusters we will have (no pre-set k). Furthermore HDBSCAN labels the low density points as noise. This is helpful for us because we do excpect that the majority of technologies is a standalone technology and can not be grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 23\n",
      "Singleton points: 45\n",
      "Cluster 0: GitOps, Low-Code And No-Code Platforms\n",
      "Cluster 1: Metaverse for mental health, The Metaverse\n",
      "Cluster 2: Immersive Technology for the Built Worls, Immersive-Reality Technologies\n",
      "Cluster 3: cloud-native, Cloud and Edge Computing\n",
      "Cluster 4: Carbon-capturing Microbes, Decarbonization Technologies, Sustainable computing, Clean Energy Generation and Storage, Electrification and Renewables, Climate Technologies Beyond Electrification\n",
      "Cluster 5: Digital Immune System, Privacy-Enhancing Technologies, Data Privacy, Data Security, and Cybersecurity Technologies, Digital Trust and Cybersecurity, Blockchain\n",
      "Cluster 6: Disease-Diagnosing Breath Sensors, Energy Harvesting from Wireless Signals, Wireless Biomarker Devices\n",
      "Cluster 7: Semiconductors and Microelectronics, Implantable Microchips\n",
      "Cluster 8: Flexible batteries, Flexible neural electronics, Solid-State Batteries\n",
      "Cluster 9: Spatial Computing, Spatial omics\n",
      "Cluster 10: Homomorphic Encryption, Quantum Information and Enabling Technologies, Quantum technologies, Quantum Computing, Quantum Machine Learning\n",
      "Cluster 11: Federated Machine Learning, Industrializing Machine Learning\n",
      "Cluster 12: Advanced Engineering Materials, Advanced Gas Turbine Engine Technologies\n",
      "Cluster 13: GenAI-Powered Healthcare Admin, GenUI\n",
      "Cluster 14: Advanced Computing, Advanced Manufacturing, Advanced Connectivity\n",
      "Cluster 15: Integrated Sensing and Communcation, Integrated Communication and Networking Technologies\n",
      "Cluster 16: Space-Based Connectivity, LEO Satellite Networks\n",
      "Cluster 17: Biotechnologies, Future of Bioengineering\n",
      "Cluster 18: Space Technologies and Systems, Future of Mobility, Future of Space Technologies\n",
      "Cluster 19: AI-facilitated healthcare, AI-Driven Diagnostics\n",
      "Cluster 20: AI for Scientific Discovery, Autonomous Agents, Artificial Intelligence, AI Copilots\n",
      "Cluster 21: Generative artificial intelligence, Generative AI\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import hdbscan\n",
    "\n",
    "# Load embeddings and DataFrame which we created earlier\n",
    "embeddings = np.load('../data/technologies-data/tech_embeddings.npy')\n",
    "df = pd.read_parquet(\"../data/technologies-data/tech_terms_enriched.parquet\")\n",
    "\n",
    "# Run HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    metric=\"euclidean\",\n",
    "    min_cluster_size=2,\n",
    "    min_samples=1,\n",
    "    cluster_selection_method=\"leaf\",\n",
    "    prediction_data=False,\n",
    ")\n",
    "\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# convert noise (-1) to unique singleton cluster IDs\n",
    "current_max = labels.max() + 1\n",
    "singelton_ids = []\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if label == -1:  # noise\n",
    "        singelton_ids.append(current_max)\n",
    "        current_max += 1\n",
    "        singelton_ids.append(label)\n",
    "        \n",
    "df[\"cluster_id\"] = labels\n",
    "\n",
    "\n",
    "singleton_count = (labels == -1).sum()\n",
    "print(f\"Number of clusters found: {df['cluster_id'].nunique()}\")\n",
    "print(f\"Singleton points: {singleton_count}\")\n",
    "\n",
    "# Get non-singleton clusters (where cluster_id != -1)\n",
    "non_singleton_clusters = df[df['cluster_id'] != -1].groupby('cluster_id')['Technology Name'].agg(list)\n",
    "\n",
    "# Print each cluster\n",
    "for cluster_id, techs in non_singleton_clusters.items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(techs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Manually create Terms for Groups\n",
    "\n",
    "As one can see we got 23 Clusters and 45 singeltons (45 categories could not be grouped). Now we manually name the clusters. We add some more clusters in case the categorization is too inclusive\n",
    "0. DevOps\n",
    "1. Metaverse\n",
    "2. Immersive-Reality Technologies\n",
    "3. Cloud Computing\n",
    "4. Climate Technologies\n",
    "5. Cybersecurity\n",
    "6. Digital Immune System\n",
    "7. Wireless Biomarker Devices\n",
    "8. Advanced Semiconductors and Microelectronics\n",
    "9. Advanced Batteries\n",
    "10. Spatial Computing \n",
    "11. Quantum Technologies\n",
    "12. Machine Learning\n",
    "13. Advanced Engineering Materials\n",
    "14. GenUI\n",
    "15. AI-facilitated healthcare\n",
    "16. Advanced Computing\n",
    "17. Advanced Manufacturing\n",
    "18. Advanced Connectivity\n",
    "19. Integrated Communication and Networking Technologies\n",
    "20. Spaced Based Connectivity\n",
    "21. Biotechnologies\n",
    "22. Future of Mobility\n",
    "23. Future of Space Technologies\n",
    "24. AI-Driven Diagnostics\n",
    "25. AI for Scientific Discorvery\n",
    "26. Autonomous Agents\n",
    "27. Generative Artificial Intelligence\n",
    "28. AI Copilots\n",
    "\n",
    "Now we combine this with our singletons and our list is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has 74 technologies\n"
     ]
    }
   ],
   "source": [
    "cluster_groups = [\n",
    "\"DevOps\",\n",
    "\"Metaverse\",\n",
    "\"Immersive-Reality Technologies\",\n",
    "\"Cloud Computing\",\n",
    "\"Climate Technologies\",\n",
    "\"Cybersecurity\",\n",
    "\"Digital Immune System\",\n",
    "\"Wireless Biomarker Devices\",\n",
    "\"Advanced Semiconductors and Microelectronics\",\n",
    "\"Advanced Batteries\",\n",
    "\"Spatial Computing \",\n",
    "\"Quantum Technologies\",\n",
    "\"Machine Learning\",\n",
    "\"Advanced Engineering Materials\",\n",
    "\"GenUI\",\n",
    "\"AI-facilitated healthcare\",\n",
    "\"Advanced Computing\",\n",
    "\"Advanced Manufacturing\",\n",
    "\"Advanced Connectivity\",\n",
    "\"Integrated Communication and Networking Technologies\",\n",
    "\"Spaced Based Connectivity\",\n",
    "\"Biotechnologies\",\n",
    "\"Future of Mobility\",\n",
    "\"Future of Space Technologies\",\n",
    "\"AI-Driven Diagnostics\",\n",
    "\"AI for Scientific Discorvery\",\n",
    "\"Autonomous Agents\",\n",
    "\"Generative Artificial Intelligence\",\n",
    "\"AI Copilots\",\n",
    "]\n",
    "singletons = df[df['cluster_id'] == -1]['Technology Name'].tolist()\n",
    "pd.DataFrame(cluster_groups + singletons, columns=['Technology Name']).to_csv('../data/technologies-data/finished_technologies.csv', index=False)\n",
    "print(f\"CSV file has {len(cluster_groups + singletons)} technologies\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
